% !TeX program = tectonic
\documentclass{beamer}

% Theme
\usetheme{Madrid}
\usecolortheme{default}
\setbeamertemplate{navigation symbols}{}
% Allow frame titles to wrap with proper spacing and positioning
\setbeamerfont{frametitle}{size=\large}
\setbeamertemplate{frametitle}{%
  \nointerlineskip
  \begin{beamercolorbo      % Normal curve fitted
      \draw[domain=-3:3, smooth, thick, blue] plot (\x, {1.2/sqrt(2*3.14159)*exp(-0.5*\x*\x)});[wd=\paperwidth,sep=0.3cm,left]{frametitle}%
    \usebeamerfont{frametitle}\insertframetitle\par
  \end{beamercolorbox}%
}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath, amssymb, bm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{arydshln}
\usepackage{hyperref}
\usepackage{centernot}
\usepackage{mathtools}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,calc}

% Ensure figures auto-fit within slides by default (consistent with Lesson 1)
\setkeys{Gin}{width=\linewidth,height=0.62\textheight,keepaspectratio}

% Sanitize PDF bookmarks: replace math/special macros with plain-text fallbacks (consistent with Lesson 1)
\pdfstringdefDisableCommands{%%
  \def\textemdash{-}%%
  \def\P{P}%%
  \def\E{E}%%
  \def\Var{Var}%%
  \def\ell{ell}%%
  \def\mathbb#1{#1}%%
  \def\mathcal#1{#1}%%
  \def\bm#1{#1}%%
}

% Notation and helpers (aligned with Lessons 00/01)
\newcommand{\R}{\mathbb{R}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\MSE}{\operatorname{MSE}}
\newcommand{\Bias}{\operatorname{Bias}}
\newcommand{\1}{\mathbf{1}}
\newcommand{\indep}{\perp\!\!\!\perp}
\newcommand{\toP}{\xrightarrow{\,\mathsf{P}\,}}
\newcommand{\toas}{\xrightarrow{\,\mathsf{a.s.}\,}}
\newcommand{\tod}{\xrightarrow{\,\mathcal{D}\,}}
\newcommand{\I}{I}

% Helper for robust top-line commands
\newcommand{\robustcmd}[1]{\csname #1\endcsname}
% Provide a safe wrapper for text-formatting commands that may appear at line start
% Use \robustcmd{textbf}{...} where editors can strip a leading backslash accidentally.
\providecommand{\textbf}[1]{\robustcmd{textbf}{#1}}

% Show a mini table of contents at the beginning of each section
\AtBeginSection[]{
  \begin{frame}{Outline}
    \robustcmd{tableofcontents}[currentsection,hideothersubsections]
  \end{frame}
}

\robustcmd{title}{Lesson 3 --- Estimator Properties}
\robustcmd{subtitle}{Consistency, Bias, Variance, Confidence Intervals}
\robustcmd{author}{Applied Statistics Course}
\robustcmd{date}{\today}

\begin{document}

\begin{frame}
  \robustcmd{titlepage}
\end{frame}

\section{General Properties of Estimators}

\begin{frame}{Why study estimator properties?}
  \textbf{Not all estimators are equal} $\to$ need tools to choose the "best" one.
  \begin{itemize}
    \item Helps decide between MLE, MoM, or others in practice
    \item Want systematic way to evaluate and compare estimators
  \end{itemize}
  \medskip
  \begin{center}
    \begin{tikzpicture}[scale=0.8]
      % Multiple estimators competing
      \draw[thick, blue] (0,0) circle (0.4) node {$\hat{\theta}_1$};
      \draw[thick, green!60!black] (2,0) circle (0.4) node {$\hat{\theta}_2$};
      \draw[thick, orange] (4,0) circle (0.4) node {$\hat{\theta}_3$};
      \node at (2,1.2) {Which one is better?};
      \draw[thick, red] (1.5,1) -- (2.5,1);
      \draw[thick, red] (1.75,1.15) -- (2.25,1.15);
      \draw[thick, red] (2,1.3) circle (0.1);
    \end{tikzpicture}
  \end{center}
\end{frame}

\begin{frame}{Bias}
  \begin{block}{Definition}
    $\Bias(\hat{\theta}) = \E[\hat{\theta}] - \theta$
  \end{block}
  \begin{itemize}
    \item \textbf{Intuition}: systematic deviation from the truth
    \item \textbf{Example}: variance estimator with divisor $n$ is biased:
    $$\E\left[\frac{1}{n}\sum_{i=1}^n (X_i - \bar{X})^2\right] = \frac{n-1}{n}\sigma^2 \neq \sigma^2$$
  \end{itemize}
  \begin{center}
    \begin{tikzpicture}[scale=0.8]
      \draw[->] (-2,0) -- (3,0) node[right] {$\theta$};
      \draw[->] (0,-0.5) -- (0,2) node[above] {density};
      % True parameter
      \draw[dashed, red, thick] (0,0) -- (0,1.8) node[above] {$\theta$};
      % Biased estimator distribution shifted right
      \draw[domain=-1.5:2.5, smooth, thick, blue] plot (\x, {0.8*exp(-(\x-0.8)*(\x-0.8))});
      \node[blue] at (1.5,1.2) {Biased estimator};
      \node[blue] at (1.5,1.0) {$\E[\hat{\theta}] \neq \theta$};
    \end{tikzpicture}
  \end{center}
\end{frame}

\begin{frame}{Variance}
  \begin{block}{Definition}
    $\Var(\hat{\theta}) = \E[(\hat{\theta} - \E[\hat{\theta}])^2]$
  \end{block}
  \begin{itemize}
    \item \textbf{Intuition}: how much the estimator fluctuates around its mean
    \item \textbf{Example}: variance of sample mean decreases with $n$:
    $$\Var(\bar{X}) = \frac{\sigma^2}{n} \to 0 \text{ as } n \to \infty$$
  \end{itemize}
  \begin{center}
    \begin{tikzpicture}[scale=0.8]
      \draw[->] (-3,0) -- (3,0) node[right] {$\theta$};
      \draw[->] (0,-0.5) -- (0,2.2) node[above] {density};
      % True parameter
      \draw[dashed, red, thick] (0,0) -- (0,2) node[above] {$\theta$};
      % High variance estimator (wide)
      \draw[domain=-3:3, smooth, thick, blue] plot (\x, {0.4*exp(-0.5*\x*\x)});
      % Low variance estimator (narrow)
      \draw[domain=-3:3, smooth, thick, green!60!black] plot (\x, {0.8*exp(-4*\x*\x)});
      \node[blue] at (-1.8,1.0) {High variance};
      \node[green!60!black] at (1.5,1.5) {Low variance};
    \end{tikzpicture}
  \end{center}
\end{frame}

\begin{frame}{Mean Squared Error (MSE) \& Bias--Variance Tradeoff}
  \begin{block}{MSE Decomposition}
    $\MSE(\hat{\theta}) = \Bias(\hat{\theta})^2 + \Var(\hat{\theta})$
  \end{block}
  \begin{itemize}
    \item MSE combines \textcolor{red}{systematic error} (bias$^2$) and \textcolor{blue}{random error} (variance)
    \item \textbf{Tradeoff}: reducing bias may increase variance, and vice versa
  \end{itemize}
  \begin{center}
    \begin{tikzpicture}[scale=0.9]
      \draw[->] (0,0) -- (4,0) node[right] {Bias$^2$};
      \draw[->] (0,0) -- (0,3) node[above] {Variance};
      % Low MSE region (bottom-left)
      \fill[blue!20] (0,0) -- (1.2,0) -- (1.2,1) -- (0,1) -- cycle;
      \node at (0.6,0.5) {\footnotesize Low MSE};
      % High MSE region (top-right)
      \fill[red!20] (2.5,1.8) -- (3.8,1.8) -- (3.8,2.8) -- (2.5,2.8) -- cycle;
      \node at (3.15,2.3) {\footnotesize High MSE};
      % Tradeoff curve
      \draw[thick, purple, domain=0.3:3.5] plot (\x, {1.5/\x});
      \node[purple] at (2.5,1.2) {\footnotesize Bias-Variance};
      \node[purple] at (2.5,1.0) {\footnotesize Tradeoff};
    \end{tikzpicture}
  \end{center}
\end{frame}

\begin{frame}{Consistency}
  \begin{block}{Definition}
    $\hat{\theta}_n \toP \theta$ as $n \to \infty$ \\
    (convergence in probability)
  \end{block}
  \begin{itemize}
    \item \textbf{Intuition}: more data $\Rightarrow$ estimator converges to true value
    \item \textbf{Example}: sample mean is consistent by the LLN:
    $$\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i \toP \E[X] = \mu$$
  \end{itemize}
  \begin{center}
    \begin{tikzpicture}[scale=0.8]
      \draw[->] (-2.5,0) -- (2.5,0) node[right] {$\theta$};
      \draw[->] (0,-0.3) -- (0,2.5) node[above] {density};
      % True parameter
      \draw[dashed, red, thick] (0,0) -- (0,2.3) node[above] {$\theta$};
      % Sequence of estimators getting narrower
      \foreach \i/\h/\col in {1/1.5/blue, 2/0.8/green!60!black, 3/0.4/orange} {
        \draw[domain=-2.5:2.5, smooth, thick, \col] plot (\x, {\h*exp(-\i*\x*\x)});
      }
      \node[blue] at (-1.8,1.8) {$n$ small};
      \node[green!60!black] at (1.5,1.2) {$n$ medium};
      \node[orange] at (1.8,0.6) {$n$ large};
    \end{tikzpicture}
  \end{center}
\end{frame}

\begin{frame}{Asymptotic Normality}
  \begin{block}{Definition}
    $\sqrt{n}(\hat{\theta} - \theta) \tod N(0, \sigma^2)$ as $n \to \infty$ \\
    (convergence in distribution)
  \end{block}
  \begin{itemize}
    \item \textbf{Intuition}: for large $n$, estimator looks Gaussian around truth
    \item \textbf{Links to CLT}: many estimators satisfy CLT-type results
    \item Enables \textbf{confidence intervals}: $\hat{\theta} \pm z_{\alpha/2} \cdot \text{SE}(\hat{\theta})$
  \end{itemize}
  \begin{center}
    \begin{tikzpicture}[scale=0.8]
      \draw[->] (-3,0) -- (3,0) node[right] {$\hat{\theta}$};
      \draw[->] (0,-0.3) -- (0,2) node[above] {density};
      % True parameter
      \draw[dashed, red, thick] (0,0) -- (0,1.8) node[above] {$\theta$};
      % Bell curve (asymptotic normal distribution)
      \draw[domain=-3:3, smooth, thick, blue] plot (\x, {1.4/sqrt(2*3.14159)*exp(-0.5*\x*\x)});
      \node[blue] at (1.8,1.2) {Asymptotic};
      \node[blue] at (1.8,1.0) {Normal};
      \node[blue] at (1.8,0.8) {Distribution};
    \end{tikzpicture}
  \end{center}
\end{frame}

\begin{frame}{Efficiency and Cramér--Rao Bound}
  \begin{block}{Definitions}
    \begin{itemize}
      \item \textbf{Efficient estimator}: smallest variance among unbiased estimators
      \item \textbf{Cramér--Rao lower bound}: $\Var(\hat{\theta}) \geq \frac{1}{I(\theta)}$
      \item \textbf{Fisher information}: $I(\theta) = \E\left[\left(\frac{\partial \log f(X;\theta)}{\partial \theta}\right)^2\right]$
    \end{itemize}
  \end{block}
  \begin{itemize}
    \item Fisher information $I(\theta)$ measures \textbf{sharpness} of likelihood
    \item Higher $I(\theta)$ $\Rightarrow$ lower variance bound
  \end{itemize}
  \begin{center}
    \begin{tikzpicture}[scale=0.8]
      \draw[->] (-2.5,0) -- (2.5,0) node[right] {$\theta$};
      \draw[->] (0,-0.3) -- (0,2) node[above] {likelihood};
      % Sharp likelihood (high Fisher information)
      \draw[domain=-2:2, smooth, thick, blue] plot (\x, {1.6*exp(-4*\x*\x)});
      % Flat likelihood (low Fisher information)
      \draw[domain=-2.5:2.5, smooth, thick, red] plot (\x, {0.8*exp(-0.5*\x*\x)});
      \node[blue] at (1.5,1.4) {Sharp};
      \node[blue] at (1.5,1.2) {(high $I(\theta)$)};
      \node[red] at (-1.8,1.0) {Flat};
      \node[red] at (-1.8,0.8) {(low $I(\theta)$)};
    \end{tikzpicture}
  \end{center}
\end{frame}

\section{Properties of MLE Estimators}

\begin{frame}{Properties of MLE}
  \textbf{Maximum Likelihood Estimators have excellent asymptotic properties:}
  \begin{itemize}
    \item \textcolor{green!60!black}{\checkmark} \textbf{Consistent}: $\hat{\theta}_{\text{MLE}} \toP \theta$ as $n \to \infty$
    \item \textcolor{green!60!black}{\checkmark} \textbf{Asymptotically Normal}: $\sqrt{n}(\hat{\theta}_{\text{MLE}} - \theta) \tod N(0, I(\theta)^{-1})$
    \item \textcolor{green!60!black}{\checkmark} \textbf{Asymptotically Efficient}: achieves Cramér--Rao lower bound
    \item \textcolor{green!60!black}{\checkmark} \textbf{Invariance property}: if $\hat{\theta}_{\text{MLE}}$ is MLE of $\theta$, then $g(\hat{\theta}_{\text{MLE}})$ is MLE of $g(\theta)$
  \end{itemize}
  \begin{center}
    \begin{tikzpicture}[scale=0.6]
      % Crown icon for "gold standard"
      \draw[thick, orange] (0,0.5) -- (-0.3,0) -- (-0.15,0) -- (-0.15,-0.3) -- (0.15,-0.3) -- (0.15,0) -- (0.3,0) -- cycle;
      \draw[thick, orange] (-0.2,0.5) -- (0,0.8) -- (0.2,0.5);
      \draw[thick, orange] (-0.1,0.5) -- (0,0.7) -- (0.1,0.5);
      \node at (0,1.2) {\textbf{\textcolor{orange}{MLE = Gold Standard}}};
    \end{tikzpicture}
  \end{center}
\end{frame}

\section{Properties of MoM Estimators}

\begin{frame}{Properties of MoM}
  \textbf{Method of Moments estimators:}
  \begin{itemize}
    \item \textcolor{green!60!black}{\checkmark} \textbf{Consistent} (from LLN on sample moments)
    \item \textcolor{green!60!black}{\checkmark} \textbf{Asymptotically Normal} (via Delta method)
    \item \textcolor{green!60!black}{\checkmark} \textbf{Easy to compute}: solve equations $m_k = \mu_k(\theta)$
    \item \textcolor{red}{\times} \textbf{Not efficient} in general (higher variance than MLE)
    \item \textcolor{red}{\times} May give \textbf{nonsensical estimates} (e.g., negative variance)
    \item \textcolor{red}{\times} \textbf{Sensitive to outliers} (uses sample moments)
  \end{itemize}
  \begin{center}
    \begin{tikzpicture}[scale=0.7]
      % Simple but weaker structure illustration
      \draw[thick, blue] (0,0) rectangle (2,1);
      \draw[thick, dashed, red] (0.2,0.2) rectangle (1.8,0.8);
      \node at (1,0.5) {\footnotesize Simple but};
      \node at (1,0.3) {\footnotesize weaker guarantees};
    \end{tikzpicture}
  \end{center}
\end{frame}

\section{Examples}

\begin{frame}{Example 1: Normal Variance}
  \textbf{MLE vs MoM for Normal variance}
  \begin{itemize}
    \item \textbf{MLE}: $\hat{\sigma}^2_{\text{MLE}} = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2$
    \item \textbf{MoM}: $\hat{\sigma}^2_{\text{MoM}} = m_2 - (\bar{X})^2 = \frac{1}{n} \sum_{i=1}^n X_i^2 - (\bar{X})^2$
    \item In this case, equal results but derived differently:
    $$\hat{\sigma}^2_{\text{MLE}} = \hat{\sigma}^2_{\text{MoM}}$$
  \end{itemize}
  \begin{center}
    \begin{tikzpicture}[scale=0.8]
      \draw[->] (-3,0) -- (3,0) node[right] {$x$};
      \draw[->] (0,-0.3) -- (0,1.8) node[above] {density};
      % Normal curve fitted
      \draw[domain=-3:3, smooth, thick, blue] plot (\x, {1.4/sqrt(2*pi)*exp(-0.5*(\x)^2)});
      \node[blue] at (1.8,1.2) {Normal curve};
      \node[blue] at (1.8,1.0) {fitted via MLE = MoM};
    \end{tikzpicture}
  \end{center}
\end{frame}

\begin{frame}{Example 2: Poisson Parameter $\lambda$}
  \textbf{For Poisson distribution}
  \begin{itemize}
    \item Both MoM and MLE give $\hat{\lambda} = \bar{X}$
    \item Example of \textbf{coincidence} between methods
    \item This happens when $\E[X] = \lambda$ is the only parameter
  \end{itemize}
  \begin{center}
    \begin{tikzpicture}[scale=0.8]
      \draw[->] (0,0) -- (6,0) node[right] {$k$};
      \draw[->] (0,0) -- (0,2) node[above] {$P(X=k)$};
      % Poisson-like histogram bars
      \foreach \k/\h in {1/0.3, 2/0.8, 3/1.5, 4/1.0, 5/0.5} {
        \draw[thick, blue, fill=blue!20] (\k-0.2,0) rectangle (\k+0.2,\h);
      }
      \node at (3,1.8) {Poisson histogram};
      \node at (3,1.6) {$\hat{\lambda}_{\text{MLE}} = \hat{\lambda}_{\text{MoM}} = \bar{X}$};
    \end{tikzpicture}
  \end{center}
\end{frame}

\begin{frame}{Example 3: Exponential Distribution}
  \textbf{For Exponential($\lambda$) distribution}
  \begin{itemize}
    \item \textbf{MLE}: $\hat{\lambda}_{\text{MLE}} = \frac{1}{\bar{X}}$ (reciprocal of sample mean)
    \item \textbf{MoM}: $\hat{\lambda}_{\text{MoM}} = \frac{1}{\bar{X}}$ (same result!)
    \item But in general, MLE may be more efficient (lower asymptotic variance)
  \end{itemize}
  \begin{center}
    \begin{tikzpicture}[scale=0.8]
      \draw[->] (0,0) -- (5,0) node[right] {$x$};
      \draw[->] (0,0) -- (0,2) node[above] {$f(x)$};
      % Exponential density
      \draw[domain=0:5, smooth, thick, blue] plot (\x, {1.5*exp(-\x)});
      \node at (3,1.2) {Exponential density};
      \node at (3,1.0) {$f(x) = \lambda e^{-\lambda x}$};
    \end{tikzpicture}
  \end{center}
\end{frame}

\section{Summary and Key Takeaways}

\begin{frame}{MLE vs MoM Summary}
  \begin{center}
    \renewcommand{\arraystretch}{1.3}
    \begin{tabular}{@{}lcc@{}}
      \toprule
      \textbf{Property} & \textbf{MLE} & \textbf{MoM} \\
      \midrule
      Existence & Requires likelihood & Requires finite moments \\
      Computation & Optimization required & Simple equations \\
      Consistency & Yes & Yes \\
      Asymptotic Normality & Yes & Yes \\
      Efficiency & Asymptotically efficient & Not efficient in general \\
      Robustness & Moderate & Sensitive to outliers \\
      Invariance & Yes & No \\
      \bottomrule
    \end{tabular}
  \end{center}
  \medskip
  \begin{center}
    \begin{tikzpicture}[scale=0.6]
      % MLE optimization icon
      \draw[thick, blue] (0,0) circle (0.3);
      \draw[thick, blue, ->] (0,-0.5) -- (0,0.5);
      \node[blue] at (0,-0.8) {MLE = optimization};

      % MoM equations icon
      \draw[thick, red] (3,0) -- (3.8,0);
      \draw[thick, red] (3.4,-0.3) -- (3.4,0.3);
      \node[red] at (3.9,-0.8) {MoM = equations};
    \end{tikzpicture}
  \end{center}
\end{frame}

\begin{frame}{Key Takeaways}
  \begin{itemize}
    \item Good estimator = balance bias and variance
    \item Both MLE and MoM are \textbf{consistent} and \textbf{asymptotically normal}
    \item \textbf{MLE} = gold standard in large samples (efficient)
    \item \textbf{MoM} = simple first approach (easy computation)
    \item Choosing an estimator depends on context:
    \begin{itemize}
      \item Data size (small vs large $n$)
      \item Model complexity
      \item Computational resources
      \item Robustness requirements
    \end{itemize}
  \end{itemize}
  \begin{center}
    \begin{tikzpicture}[scale=0.8]
      % Two roads illustration
      \draw[thick, blue] (0,0) -- (1.5,1) -- (3,1.5);
      \draw[thick, red] (0,0) -- (1.5,-1) -- (3,-1.5);
      \node[blue] at (3.2,1.5) {MLE road};
      \node[red] at (3.2,-1.5) {MoM road};
      \node at (0,-0.3) {Parameter};
      \node at (0,-0.6) {Estimation};
      % Arrows
      \draw[thick, ->] (2.8,1.3) -- (3.8,0.3);
      \draw[thick, ->] (2.8,-1.3) -- (3.8,-0.3);
      \node at (4.2,0) {$\hat{\theta}$};
    \end{tikzpicture}
  \end{center}
\end{frame}

\end{document}