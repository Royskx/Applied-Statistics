% Module: Confidence Intervals (CIs)
% This section applies Lesson 1 (CLT) and Lesson 2 (delta method) to construct intervals.
% Uses shared/data/ab_test_clicks.csv for examples and ci_prop_* functions from appendix.

\section{Confidence Intervals}

% Title & Objectives
\begin{frame}{Confidence Intervals}
  \begin{block}{Learning Objectives}
    \begin{itemize}
      \item Define $(1-\alpha)$ confidence intervals and interpret correctly
      \item Derive classical CIs: Normal mean, t-intervals, variance, proportions
      \item Compare proportion CI methods using A/B testing data
      \item Understand pivots and asymptotic CIs (delta method)
    \end{itemize}
  \end{block}

  \vspace{1em}
  \begin{center}
    \textit{Applies Lesson 1 (CLT) and Lesson 2 (delta method)}
  \end{center}
\end{frame}

% Definition and Interpretation
\begin{frame}{Confidence Interval Definition}
  \begin{block}{Definition}
    A $(1-\alpha)$ confidence interval for $\theta$ is an interval $[L_n, U_n]$ such that
    \[\P_{\theta}(L_n \leq \theta \leq U_n) = 1-\alpha \quad \forall \theta.\]
  \end{block}

  \begin{block}{Interpretation}
    If we construct the interval many times, it will contain the true
    parameter in $(1-\alpha) \times 100\%$ of cases. This is a statement
    about the procedure, not any particular interval.
  \end{block}

  \begin{block}{Common Misconception}
    A 95\% CI does NOT mean there's a 95\% probability that the true
    parameter is in \textit{this particular} interval. The true parameter
    is fixed; the interval is random. We're 95\% confident in the
    \textit{procedure}, not in any single interval.
  \end{block}
\end{frame}

\begin{frame}{Confidence Interval --- Visual}
  \begin{center}
    \includegraphics[width=0.7\textwidth]{figures/ci_interpretation.png}
  \end{center}
\end{frame}

% Pivotal Method
\begin{frame}{Pivotal Method}
  \begin{block}{General Approach}
    A pivot is a function $P_n(\theta, X_1,\dots,X_n)$ whose distribution
    doesn't depend on $\theta$. Common pivots:
    \begin{itemize}
      \item $Z = \sqrt{n}(\bar{X}_n - \mu)/\sigma \sim \Normal(0,1)$
      \item $T = \sqrt{n}(\bar{X}_n - \mu)/S \sim t_{n-1}$
      \item $\chi^2 = (n-1)S^2 / \sigma^2 \sim \ChiSq_{n-1}$
    \end{itemize}
  \end{block}

  \begin{block}{CI Construction}
    Find $q_{\alpha/2}, q_{1-\alpha/2}$ such that $\P(q_{\alpha/2} \leq P_n \leq q_{1-\alpha/2}) = 1-\alpha$.
    Then solve for $\theta$ in the inequality.
  \end{block}
\end{frame}

% Normal Mean (sigma known)
\begin{frame}{Normal Mean CI ($\sigma$ known)}
  \begin{block}{Z-Interval}
    For $X_i \sim \Normal(\mu, \sigma^2)$ with $\sigma$ known:
    \[\bar{X}_n \pm z_{1-\alpha/2} \frac{\sigma}{\sqrt{n}}.\]
  \end{block}

  \begin{block}{Derivation}
    \[\P\left(\bar{X}_n - z_{1-\alpha/2} \frac{\sigma}{\sqrt{n}} \leq \mu \leq \bar{X}_n + z_{1-\alpha/2} \frac{\sigma}{\sqrt{n}}\right) = 1-\alpha.\]
  \end{block}

  \begin{block}{Connection to Lesson 1}
    This follows directly from the Central Limit Theorem and justifies
    normal-based inference for large samples.
  \end{block}
\end{frame}

% Normal Mean (sigma unknown) - t-interval
\begin{frame}{Normal Mean CI ($\sigma$ unknown)}
  \begin{block}{t-Interval}
    For $X_i \sim \Normal(\mu, \sigma^2)$ with $\sigma$ unknown:
    \[\bar{X}_n \pm t_{n-1, 1-\alpha/2} \frac{S}{\sqrt{n}},\]
    where $S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X}_n)^2$.
  \end{block}

  \begin{block}{Pivot}
    The t-statistic $T = \sqrt{n}(\bar{X}_n - \mu)/S$ follows $t_{n-1}$ distribution,
    accounting for uncertainty in the standard error estimate.
  \end{block}

  \begin{block}{Small Sample Performance}
    t-intervals maintain nominal coverage even for small samples when
    normality holds, unlike normal approximations.
  \end{block}
\end{frame}

% Normal Variance CI
\begin{frame}{Normal Variance CI}
  \begin{block}{Chi-Squared Interval}
    For $X_i \sim \Normal(\mu, \sigma^2)$ with $\mu$ unknown:
    \[\left( \frac{(n-1)S^2}{\chi^2_{n-1, 1-\alpha/2}}, \frac{(n-1)S^2}{\chi^2_{n-1, \alpha/2}} \right).\]
  \end{block}

  \begin{block}{Pivot}
    The chi-squared statistic $\chi^2 = (n-1)S^2 / \sigma^2$ follows $\ChiSq_{n-1}$
    distribution, providing the basis for variance intervals.
  \end{block}

  \begin{block}{Note}
    This interval is not symmetric around $S^2$ due to the skewness
    of the chi-squared distribution.
  \end{block}
\end{frame}

% Binomial Proportion CIs
\begin{frame}{Binomial Proportion CIs}
  \begin{block}{Wald Interval}
    \[\hat{p} \pm z_{1-\alpha/2} \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}.\]
    Simple but poor coverage for small $n$ or extreme $p$.
  \end{block}

  \begin{block}{Wilson Score Interval}
    \[ \frac{\hat{p} + z^2/(2n)}{1 + z^2/n} \pm \frac{z}{1 + z^2/n} \sqrt{\frac{\hat{p}(1-\hat{p})}{n} + z^2/(4n^2)}. \]
    Better coverage but more complex formula.
  \end{block}

  \begin{block}{Connection to A/B Testing}
    Use \texttt{shared/data/ab\_test\_clicks.csv} to compare methods with real data.
  \end{block}
\end{frame}

\begin{frame}{Proportion CI Comparison --- Visual}
  \begin{center}
    \includegraphics[width=0.9\textwidth]{figures/proportion_ci_coverage.png}
  \end{center}
\end{frame}

% Asymptotic CIs via Delta Method
\begin{frame}{Asymptotic CIs via Delta Method}
  \begin{block}{Delta Method Statement}
    If $\sqrt{n}(\hat{\theta}_n - \theta) \to \Normal(0, \sigma^2)$, then
    for measurable function $g$,
    \[\sqrt{n}(g(\hat{\theta}_n) - g(\theta)) \to \Normal(0, [g'(\theta)]^2 \sigma^2).\]
  \end{block}

  \begin{block}{CI Construction}
    Approximate CI for $g(\theta)$:
    \[g(\hat{\theta}_n) \pm z_{1-\alpha/2} \sqrt{[g'(\hat{\theta}_n)]^2 \widehat{\Var}(\hat{\theta}_n)/n}.\]
  \end{block}

  \begin{block}{Connection to Lesson 2}
    The delta method extends asymptotic normality of MLEs to functions
    of parameters, enabling inference for ratios, logs, etc.
  \end{block}
\end{frame}

% Delta Method Example: Log-Odds
\begin{frame}{Delta Method Example: Log-Odds}
  \begin{block}{Problem}
    Estimate CI for log-odds $\theta = \log\left(\frac{p}{1-p}\right)$ from $\hat{p} = X/n$ where $X \sim \text{Binomial}(n, p)$.
  \end{block}

  \begin{block}{Solution Steps}
    \begin{enumerate}
      \item For proportion $\hat{p}$: $\sqrt{n}(\hat{p} - p) \to \Normal(0, p(1-p))$
      \item Transformation: $g(p) = \log\left(\frac{p}{1-p}\right)$
      \item Derivative: $g'(p) = \frac{1}{p(1-p)}$
      \item Delta method: $\sqrt{n}(g(\hat{p}) - g(p)) \to \Normal\left(0, \frac{1}{p(1-p)}\right)$
    \end{enumerate}
  \end{block}

  \begin{block}{95\% CI for Log-Odds}
    \[\log\left(\frac{\hat{p}}{1-\hat{p}}\right) \pm 1.96 \sqrt{\frac{1}{n\hat{p}(1-\hat{p})}}.\]
  \end{block}
\end{frame}

\begin{frame}{Delta Method --- Visual}
  \begin{center}
    \includegraphics[width=0.95\textwidth]{figures/delta_method_illustration.png}
  \end{center}
\end{frame}

% Practical Advice
\begin{frame}{Practical Advice}
  \begin{block}{When to Use Each Method}
    \begin{itemize}
      \item Z-intervals: Large samples, known variance
      \item t-intervals: Small samples, unknown variance, normality
      \item Wilson intervals: Proportions, especially small n or extreme p
      \item Delta method: Functions of parameters, large samples
    \end{itemize}
  \end{block}

  \begin{block}{General Guidelines}
    \begin{itemize}
      \item Check assumptions (normality, sample size)
      \item Compare interval width vs coverage
      \item Use simulation to verify performance
      \item Consider bootstrap for complex scenarios
    \end{itemize}
  \end{block}
\end{frame}

% Exercises
\begin{frame}{Exercises}
  \begin{enumerate}
    \item Derive the t-interval for Normal mean with unknown variance using the pivotal method.
    \item Show that the Wilson interval for p=0.5 and large n reduces to the Wald interval.
    \item Use A/B testing data from \texttt{shared/data/ab\_test\_clicks.csv} to compute and compare proportion CIs.
    \item Apply the delta method to construct a CI for the coefficient of variation $\sigma/\mu$.
  \end{enumerate}
\end{frame}

% Summary & References
\begin{frame}{Summary}
  \begin{block}{Key Takeaways}
    \begin{itemize}
      \item CIs quantify uncertainty in parameter estimates
      \item Pivotal method provides general CI construction framework
      \item t-intervals improve on normal approximations for small samples
      \item Wilson intervals provide better proportion coverage than Wald
      \item Delta method enables inference for parameter functions
    \end{itemize}
  \end{block}

  \begin{center}
    \textit{Foundation for hypothesis testing and decision making}
  \end{center}

  \footnotesize
  \begin{itemize}
    \item Wikipedia: Confidence interval \url{https://en.wikipedia.org/wiki/Confidence_interval}
    \item Casella \& Berger, \textit{Statistical Inference} (Chapter 9)
  \end{itemize}
\end{frame}