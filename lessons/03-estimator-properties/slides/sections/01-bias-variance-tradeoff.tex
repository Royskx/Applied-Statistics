% Module: Bias--Variance Tradeoff
% This section builds on Lesson 2 (MLE/MoM estimators) and provides foundation for confidence intervals in Lesson 3.
% Uses examples from shared/data/heights_weights_sample.csv where appropriate.

\section{Bias--Variance Tradeoff}

% Title & Objectives
\begin{frame}{Bias--Variance Tradeoff}
  \begin{block}{Learning Objectives}
    \begin{itemize}
      \item Define bias, variance, and mean squared error of estimators
      \item Explain the bias--variance decomposition: $\MSE = \Bias^2 + \Var$
      \item Apply tradeoff concepts to shrinkage estimators and sample variance
      \item Connect to Lesson 2 estimator properties and Lesson 1 sampling distributions
    \end{itemize}
  \end{block}

  \vspace{1em}
  \begin{center}
    \textit{Builds on Lesson 2 (MLE/MoM) foundations}
  \end{center}
\end{frame}

% Point Estimation Refresher (Bias, Variance, MSE)
\begin{frame}{Bias}
  \begin{block}{Definition}
    The bias of an estimator is
    \[\Bias(\hat{\theta}) = \E[\hat{\theta}] - \theta.\]
  \end{block}

  \begin{block}{Intuition}
    Bias measures systematic error: if non-zero the estimator is centered away
    from the true parameter even as we average over repeated samples. In practice
    bias affects accuracy and may require correction or a bias--variance tradeoff.
  \end{block}
\end{frame}

\begin{frame}{Bias --- Visual}
  \begin{center}
    \includegraphics[width=0.85\textwidth]{figures/bias_variance_conceptual.png}
  \end{center}
\end{frame}

\begin{frame}{Variance}
  \begin{block}{Definition}
    The variance of an estimator is
    \[\Var(\hat{\theta}) = \E\big[(\hat{\theta} - \E[\hat{\theta}])^2\big].\]
  \end{block}

  \begin{block}{Intuition}
    Variance measures randomness in the estimator across different samples.
    Low variance means repeated experiments produce similar estimates; high
    variance implies lack of precision. For practitioners, variance controls
    confidence interval width and sample size planning.
  \end{block}
\end{frame}

\begin{frame}{Variance --- Visual}
  \begin{center}
    \includegraphics[width=0.85\textwidth]{figures/bias_variance_conceptual.png}
  \end{center}
\end{frame}

\begin{frame}{MSE and Bias--Variance Tradeoff}
  \begin{block}{Definition}
    The mean squared error of an estimator is
    \[\MSE(\hat{\theta}) = \E\big[(\hat{\theta} - \theta)^2\big].\]
    It decomposes as
    \[\MSE(\hat{\theta}) = \Bias(\hat{\theta})^2 + \Var(\hat{\theta}).\]
  \end{block}

  \begin{block}{Intuition}
    MSE trades off accuracy (bias) against precision (variance). In applied work
    a small bias can be acceptable if it substantially reduces variance and
    thereby improves prediction or interval width.
  \end{block}
\end{frame}

\begin{frame}{MSE and Bias--Variance Tradeoff --- Visual}
  \begin{center}
    \includegraphics[width=1.0\linewidth]{figures/bias_variance_tradeoff.png}
  \end{center}
\end{frame}

% Example A: Shrinkage Mean Estimator
\begin{frame}{Shrinkage Estimator}
  \begin{block}{Definition}
    The shrinkage estimator combines data and prior information:
    \[\delta_\alpha = \alpha \bar{X} + (1-\alpha) \mu_0,\]
    where $\mu_0$ is a prior guess and $\alpha \in [0,1]$ controls shrinkage.
  \end{block}

  \begin{block}{Connection to Lesson 2}
    Builds on MLE/MoM estimators by showing how to incorporate prior information
    when data is limited, using heights data from \texttt{shared/data/heights\_weights\_sample.csv}.
  \end{block}
\end{frame}

\begin{frame}{Shrinkage Estimator --- Visual}
  \begin{center}
    \includegraphics[width=0.95\textwidth]{figures/bias_variance_tradeoff.png}
  \end{center}
  \vspace{-0.3cm}
  \footnotesize Panel 1 shows shrinkage effect; Panel 2 shows MSE decomposition
\end{frame}

% Example B: Sample Variance Estimators
\begin{frame}{Sample Variance Estimators}
  \begin{block}{Definition}
    Two common variance estimators (building on Lesson 2 Normal examples):
    \begin{align*}
    s^2 &= \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})^2 \quad (\text{unbiased}), \\
    \hat{\sigma}^2_{\MLE} &= \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2 \quad (\text{MLE, biased}).
    \end{align*}
  \end{block}

  \begin{block}{Tradeoff Analysis}
    The unbiased estimator has lower bias but higher variance; the MLE has
    higher bias but lower variance, illustrating the bias--variance tradeoff.
  \end{block}
\end{frame}

\begin{frame}{Sample Variance Estimators --- Visual}
  \begin{center}
    \includegraphics[width=0.95\textwidth]{figures/bias_variance_tradeoff.png}
  \end{center}
  \vspace{-0.3cm}
  \footnotesize Panel 3 shows comparison of unbiased vs MLE variance estimators
\end{frame}

% Pitfalls & Heuristics
\begin{frame}{Pitfalls \& Heuristics}
  \begin{block}{Common Pitfalls}
    \begin{itemize}
      \item Overlooking bias in small samples
      \item Assuming unbiased = better (ignores variance)
      \item Not considering the use case (prediction vs estimation)
    \end{itemize}
  \end{block}

  \begin{block}{Practical Heuristics}
    \begin{itemize}
      \item Use unbiased estimators when bias matters most
      \item Consider shrinkage when prior information is reliable
      \item Evaluate estimators on MSE for prediction tasks
      \item Check both bias and variance in simulation studies
    \end{itemize}
  \end{block}
\end{frame}

% Exercises
\begin{frame}{Exercises}
  \begin{enumerate}
    \item Derive the bias of the MLE variance estimator $\hat{\sigma}^2_{\MLE} = \frac{1}{n} \sum (X_i - \bar{X})^2$ for Normal data.
    \item Show that $\MSE(\hat{\theta}) = \Bias(\hat{\theta})^2 + \Var(\hat{\theta})$ using the definition of variance.
    \item Simulate the bias--variance tradeoff for the shrinkage estimator $\delta_\alpha = \alpha \bar{X} + (1-\alpha) \mu_0$ with $\mu_0 = 170$ using heights data.
    \item Compare MSE of $s^2$ vs $\hat{\sigma}^2_{\MLE}$ across different sample sizes $n = 5, 10, 20, 50$.
  \end{enumerate}
\end{frame}

% Summary & References
\begin{frame}{Summary}
  \begin{block}{Key Takeaways}
    \begin{itemize}
      \item Bias measures systematic error; variance measures random error
      \item $\MSE = \Bias^2 + \Var$ shows the fundamental tradeoff
      \item Unbiased estimators aren't always better (consider variance)
      \item Shrinkage can reduce variance at the cost of some bias
      \item Choice depends on context: estimation vs prediction
    \end{itemize}
  \end{block}

  \begin{center}
    \textit{Connects Lesson 2 estimators to practical performance evaluation}
  \end{center}

  \footnotesize
  \begin{itemize}
    \item Wikipedia: Bias--variance tradeoff \url{https://en.wikipedia.org/wiki/Bias\%E2\%80\%93variance_tradeoff}
    \item Casella \& Berger, \textit{Statistical Inference} (Chapter 7)
  \end{itemize}
\end{frame}