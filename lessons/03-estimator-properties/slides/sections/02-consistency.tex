% Module: Consistency of Estimators
% This section builds directly on Lesson 1 (LLN, CLT) and Lesson 2 (MLE/MoM estimators).
% References the uniform_max_consistency() function from the appendix.

\section{Consistency}

% Title & Objectives
\begin{frame}{Consistency}
  \begin{block}{Learning Objectives}
    \begin{itemize}
      \item Define consistency (convergence in probability) and strong consistency
      \item Connect consistency to the Law of Large Numbers (Lesson 1)
      \item Identify consistent vs inconsistent estimators
      \item Apply consistency concepts to MLE and MoM estimators (Lesson 2)
    \end{itemize}
  \end{block}

  \vspace{1em}
  \begin{center}
    \textit{Builds on Lesson 1 (LLN/CLT) and Lesson 2 (MLE/MoM)}
  \end{center}
\end{frame}

% Consistency Definition
\begin{frame}{Consistency}
  \begin{block}{Definition}
    An estimator $\hat{\theta}_n$ is consistent for $\theta$ if
    \[\hat{\theta}_n \xrightarrow{\;\mathsf{P}\;} \theta \qquad (n \to \infty),\]
    meaning $\forall \epsilon > 0$, $\P(|\hat{\theta}_n - \theta| > \epsilon) \to 0$.
  \end{block}

  \begin{block}{Intuition}
    With increasing sample size the estimator concentrates around the true
    value. Consistency is a minimal long-run requirement: without it an
    estimator may never learn the truth no matter how much data you collect.
  \end{block}
\end{frame}

\begin{frame}{Consistency --- Visual}
  \begin{center}
    \includegraphics[width=0.95\textwidth]{figures/consistency_demonstration.png}
  \end{center}
  \vspace{-0.3cm}
  \footnotesize Panels show sample mean convergence and visualization of consistency
\end{frame}

% Strong Consistency
\begin{frame}{Strong Consistency}
  \begin{block}{Definition}
    An estimator $\hat{\theta}_n$ is strongly consistent if
    \[\hat{\theta}_n \xrightarrow{\;\mathsf{a.s.}\;} \theta \qquad (n \to \infty),\]
    meaning $\P(\lim_{n\to\infty} \hat{\theta}_n = \theta) = 1$.
  \end{block}

  \begin{block}{Connection to Lesson 1}
    Strong consistency follows from the Strong Law of Large Numbers,
    while weak consistency follows from the Weak Law of Large Numbers.
  \end{block}
\end{frame}

% LLN and Consistency
\begin{frame}{Law of Large Numbers and Consistency}
  \begin{block}{Weak LLN $\to$ Consistency}
    For i.i.d. data with $\E[X_i] = \mu < \infty$:
    \[\bar{X}_n \xrightarrow{\;\mathsf{P}\;} \mu \quad \Rightarrow \quad \bar{X}_n \text{ is consistent for } \mu.\]
  \end{block}

  \begin{block}{Strong LLN $\to$ Strong Consistency}
    Under the same conditions:
    \[\bar{X}_n \xrightarrow{\;\mathsf{a.s.}\;} \mu \quad \Rightarrow \quad \bar{X}_n \text{ is strongly consistent for } \mu.\]
  \end{block}

  \begin{block}{Practical Implication}
    Sample means are consistent estimators of population means, justifying
    the use of $\bar{X}_n$ as an estimator for $\mu$ in large samples.
  \end{block}
\end{frame}

% Example: Sample Mean (Consistent)
\begin{frame}{Example: Sample Mean (Consistent)}
  \begin{block}{Normal Case}
    For $X_i \sim \Normal(\mu, \sigma^2)$ i.i.d.:
    \[\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i \xrightarrow{\;\mathsf{P}\;} \mu.\]
  \end{block}

  \begin{block}{Connection to Lesson 1}
    This follows directly from the Weak Law of Large Numbers and the
    Central Limit Theorem, providing the foundation for statistical inference.
  \end{block}

  \begin{block}{Why It Works}
    \begin{itemize}
      \item $\E[\bar{X}_n] = \mu$ (unbiased)
      \item $\Var(\bar{X}_n) = \sigma^2/n \to 0$ (variance vanishes)
      \item By Chebyshev: $\P(|\bar{X}_n - \mu| > \epsilon) \leq \Var(\bar{X}_n)/\epsilon^2 \to 0$
    \end{itemize}
  \end{block}
\end{frame}

% Example: Max for Uniform (Biased but Consistent)
\begin{frame}{Example: Uniform Maximum (Biased but Consistent)}
  \begin{block}{Setup}
    Let $X_i \sim \Uniform[0, \theta]$ i.i.d., and consider
    \[\hat{\theta}_n = \max\{X_1, \dots, X_n\}.\]
  \end{block}

  \begin{block}{Properties}
    \begin{itemize}
      \item $\E[\hat{\theta}_n] = \frac{n}{n+1} \theta < \theta$ (biased)
      \item $\Var(\hat{\theta}_n) \to 0$ as $n \to \infty$
      \item $\hat{\theta}_n \xrightarrow{\;\mathsf{P}\;} \theta$ (consistent)
    \end{itemize}
  \end{block}

  \begin{block}{Connection to Lesson 2}
    This estimator is the MLE for the Uniform$[0,\theta]$ distribution,
    demonstrating that biased estimators can still be consistent.
  \end{block}
\end{frame}

\begin{frame}{Uniform Maximum --- Visual}
  \begin{center}
    \includegraphics[width=0.95\textwidth]{figures/consistency_demonstration.png}
  \end{center}
  \vspace{-0.3cm}
  \footnotesize Panel 4 shows the uniform maximum estimator converging to $\theta$
\end{frame}

% Example: Inconsistent Estimator
\begin{frame}{Example: Inconsistent Estimator}
  \begin{block}{Counterexample}
    Consider using $X_1$ (the first observation) as an estimator for $\mu$:
    \[\hat{\mu}_n = X_1.\]
  \end{block}

  \begin{block}{Why Inconsistent}
    \begin{itemize}
      \item $\E[\hat{\mu}_n] = \mu$ (unbiased)
      \item $\Var(\hat{\mu}_n) = \sigma^2$ (variance doesn't decrease with $n$)
      \item $\P(|\hat{\mu}_n - \mu| > \epsilon) = \P(|X_1 - \mu| > \epsilon) \not\to 0$
    \end{itemize}
  \end{block}

  \begin{block}{Contrast with Lesson 1}
    This violates the variance vanishing requirement for consistency,
    unlike the sample mean which satisfies $\Var(\bar{X}_n) \to 0$.
  \end{block}
\end{frame}

% Pitfalls: Heavy Tails
\begin{frame}{Pitfalls: Heavy-Tailed Distributions}
  \begin{block}{LLN Failure}
    The Law of Large Numbers requires finite variance. For heavy-tailed
    distributions with $\Var(X_i) = \infty$, the sample mean may not be consistent.
  \end{block}

  \begin{block}{Example}
    For $X_i \sim$ Cauchy distribution:
    \begin{itemize}
      \item No finite mean or variance
      \item Sample mean does not converge (in probability or a.s.)
      \item Need robust alternatives (median, trimmed mean)
    \end{itemize}
  \end{block}

  \begin{block}{Practical Implication}
    Always check moment conditions before assuming consistency,
    especially with real data that may have heavy tails.
  \end{block}
\end{frame}

% Exercises
\begin{frame}{Exercises}
  \begin{enumerate}
    \item Prove that if $\hat{\theta}_n \xrightarrow{\;\mathsf{P}\;} \theta$ and $g$ is continuous, then $g(\hat{\theta}_n) \xrightarrow{\;\mathsf{P}\;} g(\theta)$.
    \item Show that the sample median is consistent for the population median under mild conditions.
    \item Use the \texttt{uniform\_max\_consistency()} function from the appendix to verify that $\max\{X_i\}$ is consistent for $\theta$ in Uniform$[0,\theta]$.
    \item Explain why $X_1$ is inconsistent for $\mu$ while $\bar{X}_n$ is consistent.
  \end{enumerate}
\end{frame}

% Summary & References
\begin{frame}{Summary}
  \begin{block}{Key Takeaways}
    \begin{itemize}
      \item Consistency requires both correct centering and vanishing variance
      \item Sample means are consistent by LLN (Lesson 1 foundation)
      \item Biased estimators can be consistent if bias vanishes
      \item MLEs and MoM estimators are typically consistent (Lesson 2)
      \item Heavy tails can break consistency assumptions
    \end{itemize}
  \end{block}

  \begin{center}
    \textit{Provides theoretical foundation for Lesson 2 estimators}
  \end{center}

  \footnotesize
  \begin{itemize}
    \item Wikipedia: Consistent estimator \url{https://en.wikipedia.org/wiki/Consistent_estimator}
    \item Casella \& Berger, \textit{Statistical Inference} (Chapter 7)
  \end{itemize}
\end{frame}