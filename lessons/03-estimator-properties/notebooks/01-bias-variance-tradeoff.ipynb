{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3 Module 1: Bias–Variance Tradeoff\n",
    "\n",
    "This notebook demonstrates the bias–variance tradeoff using practical examples.\n",
    "It builds on Lesson 2 (MLE/MoM estimators) and provides foundation for confidence intervals.\n",
    "\n",
    "## Learning Objectives\n",
    "- Define bias, variance, and mean squared error of estimators\n",
    "- Explain the bias–variance decomposition: MSE = Bias^2 + Var\n",
    "- Apply tradeoff concepts to shrinkage estimators and sample variance\n",
    "- Connect to Lesson 2 estimator properties and Lesson 1 sampling distributions\n",
    "\n",
    "## Repository Context\n",
    "- Uses datasets from `shared/data/` where appropriate\n",
    "- Connects to Lesson 1 (LLN/CLT) and Lesson 2 (MLE/MoM)\n",
    "- Uses helper functions from the appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Set style and random seed\n",
    "sns.set_theme(context=\"talk\", style=\"whitegrid\")\n",
    "sns.set_palette([\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\",\n",
    "                 \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\"])\n",
    "rng = np.random.default_rng(2025)\n",
    "\n",
    "print(\"Environment setup complete. Random seed: 2025\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bias, Variance, and MSE Definitions\n",
    "\n",
    "Let's implement the core concepts and verify the decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_estimator_properties(estimator_fn, true_theta, gen_fn, n, R=10000):\n",
    "    \"\"\"\n",
    "    Compute empirical bias, variance, and MSE for an estimator.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    estimator_fn : callable\n",
    "        Function that takes sample and returns estimate\n",
    "    true_theta : float\n",
    "        True parameter value\n",
    "    gen_fn : callable\n",
    "        Function that generates sample of size n\n",
    "    n : int\n",
    "        Sample size\n",
    "    R : int\n",
    "        Number of replications\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict with bias, variance, mse, and estimates\n",
    "    \"\"\"\n",
    "    estimates = np.empty(R)\n",
    "\n",
    "    for r in range(R):\n",
    "        sample = gen_fn(n)\n",
    "        estimates[r] = estimator_fn(sample)\n",
    "\n",
    "    bias = np.mean(estimates) - true_theta\n",
    "    variance = np.var(estimates, ddof=0)\n",
    "    mse = np.mean((estimates - true_theta)**2)\n",
    "\n",
    "    return {\n",
    "        'bias': bias,\n",
    "        'variance': variance,\n",
    "        'mse': mse,\n",
    "        'estimates': estimates,\n",
    "        'bias_squared': bias**2\n",
    "    }\n",
    "\n",
    "print(\"Function defined: compute_estimator_properties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the decomposition: MSE should equal Bias^2 + Variance\n",
    "true_mu = 5.0\n",
    "true_sigma = 2.0\n",
    "n = 10\n",
    "R = 5000\n",
    "\n",
    "def sample_mean(x):\n",
    "    return np.mean(x)\n",
    "\n",
    "def generate_normal(n):\n",
    "    return rng.normal(true_mu, true_sigma, n)\n",
    "\n",
    "props = compute_estimator_properties(sample_mean, true_mu, generate_normal, n, R)\n",
    "\n",
    "print(f\"Sample size: {n}\")\n",
    "print(f\"Bias: {props['bias']:.6f}\")\n",
    "print(f\"Variance: {props['variance']:.6f}\")\n",
    "print(f\"Bias^2: {props['bias_squared']:.6f}\")\n",
    "print(f\"MSE: {props['mse']:.6f}\")\n",
    "print(f\"Bias^2 + Var: {props['bias_squared'] + props['variance']:.6f}\")\n",
    "print(f\"Decomposition check: {abs(props['mse'] - (props['bias_squared'] + props['variance'])) < 1e-10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Shrinkage Estimator Example\n",
    "\n",
    "Using heights data from the repository to demonstrate shrinkage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load heights data\n",
    "heights_df = pd.read_csv(\"../../../shared/data/heights_weights_sample.csv\")\n",
    "heights = heights_df['height_cm'].values\n",
    "\n",
    "print(f\"Loaded {len(heights)} height measurements\")\n",
    "print(f\"Sample mean: {np.mean(heights):.2f} cm\")\n",
    "print(f\"Sample std: {np.std(heights, ddof=1):.2f} cm\")\n",
    "\n",
    "# Use population mean as prior (approximate)\n",
    "mu_0 = 170  # Prior guess for average height\n",
    "true_mu = np.mean(heights)  # Use sample mean as proxy for true mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shrinkage_estimator(x, alpha, mu_0):\n",
    "    \"\"\"Shrinkage estimator: alpha * mean(x) + (1-alpha) * mu_0\"\"\"\n",
    "    return alpha * np.mean(x) + (1 - alpha) * mu_0\n",
    "\n",
    "# Compare shrinkage estimators for different alpha values\n",
    "n_small = 20  # Small sample\n",
    "alpha_values = np.linspace(0, 1, 21)\n",
    "R = 2000\n",
    "\n",
    "results = []\n",
    "for alpha in alpha_values:\n",
    "    def est_fn(x):\n",
    "        return shrinkage_estimator(x, alpha, mu_0)\n",
    "\n",
    "    props = compute_estimator_properties(est_fn, true_mu, generate_normal, n_small, R)\n",
    "    results.append({\n",
    "        'alpha': alpha,\n",
    "        'bias': props['bias'],\n",
    "        'variance': props['variance'],\n",
    "        'mse': props['mse'],\n",
    "        'bias_squared': props['bias_squared']\n",
    "    })\n",
    "\n",
    "shrinkage_df = pd.DataFrame(results)\n",
    "print(\"Computed shrinkage estimator properties for alpha in [0,1]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the tradeoff\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Bias and variance vs alpha\n",
    "axes[0,0].plot(shrinkage_df['alpha'], shrinkage_df['bias'], 'b-', linewidth=2, label='Bias')\n",
    "axes[0,0].plot(shrinkage_df['alpha'], shrinkage_df['variance'], 'r-', linewidth=2, label='Variance')\n",
    "axes[0,0].set_xlabel(r'$\\alpha$')\n",
    "axes[0,0].set_ylabel('Value')\n",
    "axes[0,0].set_title('Bias and Variance vs Shrinkage Parameter')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Bias squared vs alpha\n",
    "axes[0,1].plot(shrinkage_df['alpha'], shrinkage_df['bias_squared'], 'g-', linewidth=2)\n",
    "axes[0,1].set_xlabel(r'$\\alpha$')\n",
    "axes[0,1].set_ylabel('Bias^2')\n",
    "axes[0,1].set_title('Bias Squared vs Shrinkage Parameter')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# MSE decomposition\n",
    "axes[1,0].plot(shrinkage_df['alpha'], shrinkage_df['bias_squared'], 'g-', linewidth=2, label='Bias^2')\n",
    "axes[1,0].plot(shrinkage_df['alpha'], shrinkage_df['variance'], 'r-', linewidth=2, label='Variance')\n",
    "axes[1,0].plot(shrinkage_df['alpha'], shrinkage_df['mse'], 'b-', linewidth=3, label='MSE')\n",
    "axes[1,0].set_xlabel(r'$\\alpha$')\n",
    "axes[1,0].set_ylabel('Value')\n",
    "axes[1,0].set_title('MSE Decomposition')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Find optimal alpha\n",
    "optimal_idx = shrinkage_df['mse'].argmin()\n",
    "optimal_alpha = shrinkage_df.iloc[optimal_idx]['alpha']\n",
    "axes[1,0].axvline(optimal_alpha, color='black', linestyle='--', alpha=0.7)\n",
    "# axes[1,0].text(optimal_alpha + 0.05, 0.5, f'Optimal alpha = {optimal_alpha:.2f}',\n",
    "#                bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "\n",
    "# MSE vs alpha\n",
    "axes[1,1].plot(shrinkage_df['alpha'], shrinkage_df['mse'], 'b-', linewidth=3)\n",
    "axes[1,1].axvline(optimal_alpha, color='black', linestyle='--', alpha=0.7)\n",
    "axes[1,1].set_xlabel(r'$\\alpha$')\n",
    "axes[1,1].set_ylabel('MSE')\n",
    "axes[1,1].set_title('MSE vs Shrinkage Parameter')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../slides/figures/bias_variance_tradeoff.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Optimal shrinkage parameter: {optimal_alpha:.3f}\")\n",
    "print(f\"Minimum MSE: {shrinkage_df['mse'].min():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sample Variance Estimators Comparison\n",
    "\n",
    "Compare the unbiased and MLE variance estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unbiased_variance(x):\n",
    "    \"\"\"Unbiased sample variance (denominator n-1)\"\"\"\n",
    "    return np.var(x, ddof=1)\n",
    "\n",
    "def mle_variance(x):\n",
    "    \"\"\"MLE sample variance (denominator n)\"\"\"\n",
    "    return np.var(x, ddof=0)\n",
    "\n",
    "# Compare across different sample sizes\n",
    "sample_sizes = [5, 10, 20, 50, 100]\n",
    "R = 5000\n",
    "true_sigma2 = 4.0  # True variance\n",
    "\n",
    "comparison_results = []\n",
    "for n in sample_sizes:\n",
    "    # Unbiased estimator\n",
    "    props_unbiased = compute_estimator_properties(\n",
    "        unbiased_variance, true_sigma2, generate_normal, n, R\n",
    "    )\n",
    "\n",
    "    # MLE estimator\n",
    "    props_mle = compute_estimator_properties(\n",
    "        mle_variance, true_sigma2, generate_normal, n, R\n",
    "    )\n",
    "\n",
    "    comparison_results.append({\n",
    "        'n': n,\n",
    "        'unbiased_bias': props_unbiased['bias'],\n",
    "        'unbiased_var': props_unbiased['variance'],\n",
    "        'unbiased_mse': props_unbiased['mse'],\n",
    "        'mle_bias': props_mle['bias'],\n",
    "        'mle_var': props_mle['variance'],\n",
    "        'mle_mse': props_mle['mse']\n",
    "    })\n",
    "\n",
    "var_comparison_df = pd.DataFrame(comparison_results)\n",
    "print(\"Computed variance estimator comparison across sample sizes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Bias comparison\n",
    "axes[0,0].plot(var_comparison_df['n'], var_comparison_df['unbiased_bias'], 'b-', linewidth=2, label='Unbiased')\n",
    "axes[0,0].plot(var_comparison_df['n'], var_comparison_df['mle_bias'], 'r-', linewidth=2, label='MLE')\n",
    "axes[0,0].axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "axes[0,0].set_xlabel('Sample Size')\n",
    "axes[0,0].set_ylabel('Bias')\n",
    "axes[0,0].set_title('Bias Comparison')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Variance comparison\n",
    "axes[0,1].plot(var_comparison_df['n'], var_comparison_df['unbiased_var'], 'b-', linewidth=2, label='Unbiased')\n",
    "axes[0,1].plot(var_comparison_df['n'], var_comparison_df['mle_var'], 'r-', linewidth=2, label='MLE')\n",
    "axes[0,1].set_xlabel('Sample Size')\n",
    "axes[0,1].set_ylabel('Variance')\n",
    "axes[0,1].set_title('Variance Comparison')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# MSE comparison\n",
    "axes[1,0].plot(var_comparison_df['n'], var_comparison_df['unbiased_mse'], 'b-', linewidth=2, label='Unbiased')\n",
    "axes[1,0].plot(var_comparison_df['n'], var_comparison_df['mle_mse'], 'r-', linewidth=2, label='MLE')\n",
    "axes[1,0].set_xlabel('Sample Size')\n",
    "axes[1,0].set_ylabel('MSE')\n",
    "axes[1,0].set_title('MSE Comparison')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# MSE decomposition for n=20\n",
    "n_idx = 2  # n=20\n",
    "unbiased_mse = var_comparison_df.iloc[n_idx]['unbiased_mse']\n",
    "unbiased_bias2 = var_comparison_df.iloc[n_idx]['unbiased_bias']**2\n",
    "unbiased_var = var_comparison_df.iloc[n_idx]['unbiased_var']\n",
    "\n",
    "mle_mse = var_comparison_df.iloc[n_idx]['mle_mse']\n",
    "mle_bias2 = var_comparison_df.iloc[n_idx]['mle_bias']**2\n",
    "mle_var = var_comparison_df.iloc[n_idx]['mle_var']\n",
    "\n",
    "x_pos = var_comparison_df.iloc[n_idx]['n']\n",
    "axes[1,1].bar([x_pos-2, x_pos+2], [unbiased_bias2, mle_bias2], width=1.5, label='Bias^2')\n",
    "axes[1,1].bar([x_pos-2, x_pos+2], [unbiased_var, mle_var], width=1.5, bottom=[unbiased_bias2, mle_bias2], label='Variance')\n",
    "axes[1,1].axhline(unbiased_mse, color='blue', linestyle='--', alpha=0.7)\n",
    "axes[1,1].axhline(mle_mse, color='red', linestyle='--', alpha=0.7)\n",
    "axes[1,1].set_xlabel('Estimator (n=20)')\n",
    "axes[1,1].set_ylabel('MSE Components')\n",
    "axes[1,1].set_title('MSE Decomposition (n=20)')\n",
    "axes[1,1].set_xticks([x_pos-2, x_pos+2])\n",
    "axes[1,1].set_xticklabels(['Unbiased', 'MLE'])\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../slides/figures/variance_estimators_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Variance estimator comparison complete\")\n",
    "print(f\"For n=20: Unbiased MSE = {unbiased_mse:.6f}, MLE MSE = {mle_mse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simulation Exercises\n",
    "\n",
    "These simulations correspond to Exercises 3 and 4 from the workbook and extend the notebook with reproducible Monte Carlo analyses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Shrinkage Estimator Bias–Variance Tradeoff\n",
    "\n",
    "We investigate how the shrinkage parameter $\\alpha$ balances bias and variance when blending the sample mean with a prior guess $\\mu_0 = 170$ for the heights data. Each point below summarises 1,000 bootstrap samples of size $n = 30$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo simulation for Exercise 3\n",
    "sim_rng = np.random.default_rng(2025)\n",
    "alphas = np.linspace(0.0, 1.0, 21)\n",
    "n_samples = 30\n",
    "n_sims = 1000\n",
    "\n",
    "records = []\n",
    "for alpha in alphas:\n",
    "    samples = sim_rng.choice(heights, size=(n_sims, n_samples), replace=True)\n",
    "    sample_means = samples.mean(axis=1)\n",
    "    estimates = alpha * sample_means + (1.0 - alpha) * mu_0\n",
    "\n",
    "    bias = estimates.mean() - true_mu\n",
    "    variance = estimates.var()\n",
    "    mse = np.mean((estimates - true_mu) ** 2)\n",
    "\n",
    "    records.append({\n",
    "        'alpha': alpha,\n",
    "        'bias': bias,\n",
    "        'bias_squared': bias ** 2,\n",
    "        'variance': variance,\n",
    "        'mse': mse\n",
    "    })\n",
    "\n",
    "shrinkage_sim_df = pd.DataFrame(records)\n",
    "shrinkage_sim_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(shrinkage_sim_df['alpha'], shrinkage_sim_df['bias_squared'], marker='o', label='Bias^2')\n",
    "ax.plot(shrinkage_sim_df['alpha'], shrinkage_sim_df['variance'], marker='s', label='Variance')\n",
    "ax.plot(shrinkage_sim_df['alpha'], shrinkage_sim_df['mse'], marker='^', linewidth=2, label='MSE')\n",
    "ax.axvline(1.0, color='red', linestyle='--', alpha=0.5, label='No shrinkage (alpha = 1)')\n",
    "optimal_row = shrinkage_sim_df.loc[shrinkage_sim_df['mse'].idxmin()]\n",
    "ax.axvline(optimal_row['alpha'], color='black', linestyle=':', alpha=0.7, label=f\"Min MSE alpha = {optimal_row['alpha']:.2f}\")\n",
    "ax.set_xlabel('Shrinkage parameter alpha')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title('Bias^2, Variance, and MSE for Shrinkage Estimator')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Minimum MSE {optimal_row['mse']:.6f} achieved at alpha = {optimal_row['alpha']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** For small $\\alpha$ the estimator leans heavily on the prior and exhibits low variance but large (squared) bias. Increasing $\\alpha$ reduces the bias at the cost of higher variance, and the minimum MSE occurs at an intermediate value that balances the two sources of error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: MSE of Variance Estimators\n",
    "\n",
    "We compare the unbiased sample variance $s^2$ and the MLE variance estimator across sample sizes $n = 5, 10, 20, 50$ using normal samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo simulation for Exercise 4\n",
    "sim_rng = np.random.default_rng(2026)\n",
    "sample_sizes = [5, 10, 20, 50]\n",
    "n_sims = 10000\n",
    "sigma2_true = 1.0\n",
    "\n",
    "var_records = []\n",
    "for n in sample_sizes:\n",
    "    samples = sim_rng.normal(loc=0.0, scale=np.sqrt(sigma2_true), size=(n_sims, n))\n",
    "    s2_estimates = samples.var(axis=1, ddof=1)\n",
    "    mle_estimates = samples.var(axis=1, ddof=0)\n",
    "\n",
    "    var_records.append({\n",
    "        'n': n,\n",
    "        'unbiased_bias': s2_estimates.mean() - sigma2_true,\n",
    "        'unbiased_variance': s2_estimates.var(),\n",
    "        'unbiased_mse': np.mean((s2_estimates - sigma2_true) ** 2),\n",
    "        'mle_bias': mle_estimates.mean() - sigma2_true,\n",
    "        'mle_variance': mle_estimates.var(),\n",
    "        'mle_mse': np.mean((mle_estimates - sigma2_true) ** 2)\n",
    "    })\n",
    "\n",
    "variance_sim_df = pd.DataFrame(var_records)\n",
    "variance_sim_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].plot(variance_sim_df['n'], variance_sim_df['unbiased_mse'], marker='o', linewidth=2, label='s^2 (unbiased)')\n",
    "axes[0].plot(variance_sim_df['n'], variance_sim_df['mle_mse'], marker='s', linewidth=2, label='MLE (biased)')\n",
    "axes[0].set_xlabel('Sample size n')\n",
    "axes[0].set_ylabel('MSE')\n",
    "axes[0].set_title('MSE Comparison')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].legend()\n",
    "\n",
    "n_focus = 10\n",
    "focus_row = variance_sim_df.loc[variance_sim_df['n'] == n_focus].iloc[0]\n",
    "axes[1].bar(['s^2', 'MLE'], [focus_row['unbiased_bias'] ** 2, focus_row['mle_bias'] ** 2], label='Bias^2', alpha=0.7)\n",
    "axes[1].bar(['s^2', 'MLE'], [focus_row['unbiased_variance'], focus_row['mle_variance']],\n",
    "           bottom=[focus_row['unbiased_bias'] ** 2, focus_row['mle_bias'] ** 2], label='Variance', alpha=0.7)\n",
    "axes[1].set_ylabel('Contribution')\n",
    "axes[1].set_title(f'Bias-Variance Decomposition (n={n_focus})')\n",
    "axes[1].grid(True, axis='y', alpha=0.3)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaways:** The MLE variance estimator is biased downward but enjoys lower variance in small samples, often yielding a lower MSE for modest $n$. The unbiased estimator eliminates bias but pays for it with higher variability; as $n$ grows, both estimators converge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary and Key Takeaways\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. The fundamental bias-variance decomposition\n",
    "2. How shrinkage estimators trade bias for reduced variance\n",
    "3. The practical differences between unbiased and MLE variance estimators\n",
    "4. How to evaluate estimators using MSE in practice\n",
    "\n",
    "Key insights:\n",
    "- Unbiased estimators aren't always better when variance is high\n",
    "- The optimal estimator depends on the specific use case\n",
    "- Simulation is essential for understanding finite-sample properties"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "applied-stats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
