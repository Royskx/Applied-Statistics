{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3 Module 3: Asymptotic Efficiency & Cramér–Rao Lower Bound\n",
    "\n",
    "This notebook demonstrates Fisher information and the Cramér–Rao lower bound using practical examples.\n",
    "It extends Lesson 2 (Fisher information, MLE) and provides foundation for confidence intervals.\n",
    "\n",
    "## Learning Objectives\n",
    "- Define score function, Fisher information, and CRLB for unbiased estimators\n",
    "- State regularity conditions and equality cases\n",
    "- Explain asymptotic normality of MLEs and asymptotic efficiency\n",
    "- Work through Normal, Poisson, and Exponential examples (Lesson 2)\n",
    "\n",
    "## Repository Context\n",
    "- Uses `fisher_info_*` functions from the appendix\n",
    "- Extends Lesson 2 MLE properties and Fisher information concepts\n",
    "- Demonstrates efficiency of Lesson 2 estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Set style and random seed\n",
    "sns.set_theme(context=\"talk\", style=\"whitegrid\")\n",
    "sns.set_palette([\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\",\n",
    "                 \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\"])\n",
    "rng = np.random.default_rng(2025)\n",
    "\n",
    "print(\"Environment setup complete. Random seed: 2025\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fisher Information Functions\n",
    "\n",
    "Using the functions from the appendix to compute Fisher information for common distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fisher information functions (from appendix)\n",
    "def fisher_info_normal_mean(sigma):\n",
    "    \"\"\"Per-observation Fisher information for μ in Normal(μ, σ²) with σ known.\"\"\"\n",
    "    return 1.0 / (sigma**2)\n",
    "\n",
    "def fisher_info_poisson(lam):\n",
    "    \"\"\"Per-observation Fisher information for λ in Poisson(λ).\"\"\"\n",
    "    return 1.0 / lam\n",
    "\n",
    "def fisher_info_exponential(lam):\n",
    "    \"\"\"Per-observation Fisher information for λ in Exponential(rate=λ).\"\"\"\n",
    "    return 1.0 / (lam**2)\n",
    "\n",
    "print(\"Fisher information functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Fisher information for different parameters\n",
    "sigma_values = np.array([0.5, 1.0, 2.0, 5.0])\n",
    "lambda_values = np.array([0.5, 1.0, 2.0, 5.0])\n",
    "\n",
    "print(\"Fisher Information Comparison:\")\n",
    "print(\"σ\\tI_N(μ) = 1/σ²\")\n",
    "for sigma in sigma_values:\n",
    "    print(f\"{sigma}\\t{fisher_info_normal_mean(sigma):.3f}\")\n",
    "\n",
    "print(\"\\nλ\\tI_P(λ) = 1/λ\")\n",
    "for lam in lambda_values:\n",
    "    print(f\"{lam}\\t{fisher_info_poisson(lam):.3f}\")\n",
    "\n",
    "print(\"\\nλ\\tI_E(λ) = 1/λ²\")\n",
    "for lam in lambda_values:\n",
    "    print(f\"{lam}\\t{fisher_info_exponential(lam):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Normal Mean: CRLB Achievement\n",
    "\n",
    "Demonstrate that the sample mean achieves the CRLB for Normal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_crlb_achievement(true_mu, true_sigma, n_values, R=5000):\n",
    "    \"\"\"\n",
    "    Simulate CRLB achievement for Normal mean estimation.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for n in n_values:\n",
    "        estimates = np.zeros(R)\n",
    "\n",
    "        for r in range(R):\n",
    "            sample = rng.normal(true_mu, true_sigma, n)\n",
    "            estimates[r] = np.mean(sample)\n",
    "\n",
    "        empirical_var = np.var(estimates, ddof=0)\n",
    "        crlb = true_sigma**2 / n\n",
    "        efficiency_ratio = empirical_var / crlb\n",
    "\n",
    "        results.append({\n",
    "            'n': n,\n",
    "            'empirical_var': empirical_var,\n",
    "            'crlb': crlb,\n",
    "            'efficiency_ratio': efficiency_ratio\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"Function defined: simulate_crlb_achievement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate CRLB achievement\n",
    "true_mu = 10.0\n",
    "true_sigma = 2.0\n",
    "n_values = [5, 10, 20, 50, 100, 200]\n",
    "\n",
    "normal_results = simulate_crlb_achievement(true_mu, true_sigma, n_values, R=10000)\n",
    "print(\"Normal mean CRLB achievement:\")\n",
    "print(normal_results.round(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot CRLB achievement\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Variance comparison\n",
    "axes[0].plot(normal_results['n'], normal_results['empirical_var'], 'b-', linewidth=3, marker='o', label='Empirical Var')\n",
    "axes[0].plot(normal_results['n'], normal_results['crlb'], 'r-', linewidth=3, marker='s', label='CRLB')\n",
    "axes[0].set_xlabel('Sample Size')\n",
    "axes[0].set_ylabel('Variance')\n",
    "axes[0].set_title('Sample Mean Variance vs CRLB')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Efficiency ratio\n",
    "axes[1].plot(normal_results['n'], normal_results['efficiency_ratio'], 'g-', linewidth=3, marker='D')\n",
    "axes[1].axhline(1.0, color='red', linestyle='--', alpha=0.7, label='Perfect Efficiency')\n",
    "axes[1].set_xlabel('Sample Size')\n",
    "axes[1].set_ylabel('Efficiency Ratio')\n",
    "axes[1].set_title('Efficiency Ratio (Empirical/CRLB)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../slides/figures/normal_crlb_achievement.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Asymptotic efficiency ratio: {normal_results.iloc[-1]['efficiency_ratio']:.4f}\")\n",
    "print(\"Sample mean achieves CRLB asymptotically\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Poisson Rate: MLE Efficiency\n",
    "\n",
    "Demonstrate CRLB achievement for Poisson MLE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisher Information and CRLB for Selected Rates\n",
    "\n",
    "We start by verifying the analytic Fisher information $I(\\lambda)=1/\\lambda$ for a few Poisson rates and summarise the implied single-observation CRLB. A simulation check confirms the variance-of-score definition matches the analytic value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_values_focus = np.array([2.0, 5.0, 10.0])\n",
    "\n",
    "def summarise_poisson_fisher(lambda_vals, n_sims=100_000):\n",
    "    records = []\n",
    "    score_cache = {}\n",
    "    sample_cache = {}\n",
    "\n",
    "    for lam in lambda_vals:\n",
    "        analytical = fisher_info_poisson(lam)\n",
    "\n",
    "        samples = rng.poisson(lam, n_sims)\n",
    "        scores = samples / lam - 1.0\n",
    "        numerical = np.var(scores)\n",
    "\n",
    "        records.append({\n",
    "            'lambda': lam,\n",
    "            'I_analytical': analytical,\n",
    "            'I_numerical': numerical,\n",
    "            'CRLB_single': 1.0 / analytical\n",
    "        })\n",
    "\n",
    "        score_cache[lam] = scores[:2000]\n",
    "        sample_cache[lam] = samples[:2000]\n",
    "\n",
    "    summary_df = pd.DataFrame(records)\n",
    "    return summary_df, score_cache, sample_cache\n",
    "\n",
    "poisson_info_df, poisson_score_cache, poisson_sample_cache = summarise_poisson_fisher(lambda_values_focus)\n",
    "poisson_info_df.round({'I_analytical': 6, 'I_numerical': 6, 'CRLB_single': 6})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "lambda_grid = np.linspace(0.5, 15, 200)\n",
    "axes[0, 0].plot(lambda_grid, 1.0 / lambda_grid, linewidth=2, color='steelblue', label=r'$I(\\lambda) = 1/\\lambda$')\n",
    "axes[0, 0].scatter(poisson_info_df['lambda'], poisson_info_df['I_analytical'],\n",
    "                   color='crimson', s=80, zorder=5, label='Selected rates')\n",
    "axes[0, 0].set_xlabel('Poisson rate $\\lambda$')\n",
    "axes[0, 0].set_ylabel('Fisher information $I(\\lambda)$')\n",
    "axes[0, 0].set_title('Per-observation information')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].legend()\n",
    "\n",
    "axes[0, 1].plot(lambda_grid, lambda_grid, linewidth=2, color='darkorange', label='CRLB, n=1')\n",
    "axes[0, 1].plot(lambda_grid, lambda_grid / 10.0, linewidth=2, color='seagreen', linestyle='--', label='CRLB, n=10')\n",
    "axes[0, 1].scatter(poisson_info_df['lambda'], poisson_info_df['CRLB_single'], color='crimson', s=80, zorder=5)\n",
    "axes[0, 1].set_xlabel('Poisson rate $\\lambda$')\n",
    "axes[0, 1].set_ylabel('Variance lower bound')\n",
    "axes[0, 1].set_title('Cramér–Rao bounds')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].legend()\n",
    "\n",
    "for lam, scores in poisson_score_cache.items():\n",
    "    axes[1, 0].hist(scores, bins=40, density=True, alpha=0.5, label=f'λ = {lam:.0f}')\n",
    "axes[1, 0].axvline(0.0, color='black', linestyle='--', alpha=0.6)\n",
    "axes[1, 0].set_xlabel('Score $U(\\lambda) = X/\\lambda - 1$')\n",
    "axes[1, 0].set_ylabel('Density')\n",
    "axes[1, 0].set_title('Score distributions (simulated)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "for lam, samples in poisson_sample_cache.items():\n",
    "    support = np.arange(0, samples.max() + 1)\n",
    "    pmf = stats.poisson.pmf(support, lam)\n",
    "    axes[1, 1].stem(support, pmf, linefmt='-', markerfmt='o', basefmt=' ', label=f'λ = {lam:.0f}', use_line_collection=True)\n",
    "axes[1, 1].set_xlabel('$x$')\n",
    "axes[1, 1].set_ylabel('P(X = x)')\n",
    "axes[1, 1].set_title('Poisson pmfs for selected rates')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table confirms $I(\\lambda)=1/\\lambda$ and the plots visualise how the information and CRLB scale with the rate. The score histograms centre at zero and have variance matching the analytic information, reinforcing the simulation check before we examine the sampling distribution of the Poisson MLE below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_poisson_efficiency(true_lambda, n_values, R=5000):\n",
    "    \"\"\"\n",
    "    Simulate CRLB achievement for Poisson rate estimation.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for n in n_values:\n",
    "        estimates = np.zeros(R)\n",
    "\n",
    "        for r in range(R):\n",
    "            sample = rng.poisson(true_lambda, n)\n",
    "            estimates[r] = np.mean(sample)  # MLE for Poisson\n",
    "\n",
    "        empirical_var = np.var(estimates, ddof=0)\n",
    "        crlb = true_lambda / n\n",
    "        efficiency_ratio = empirical_var / crlb\n",
    "\n",
    "        results.append({\n",
    "            'n': n,\n",
    "            'empirical_var': empirical_var,\n",
    "            'crlb': crlb,\n",
    "            'efficiency_ratio': efficiency_ratio\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"Function defined: simulate_poisson_efficiency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Poisson efficiency\n",
    "true_lambda = 3.0\n",
    "poisson_results = simulate_poisson_efficiency(true_lambda, n_values, R=10000)\n",
    "print(\"Poisson rate CRLB achievement:\")\n",
    "print(poisson_results.round(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Poisson CRLB achievement\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Variance comparison\n",
    "axes[0].plot(poisson_results['n'], poisson_results['empirical_var'], 'b-', linewidth=3, marker='o', label='Empirical Var')\n",
    "axes[0].plot(poisson_results['n'], poisson_results['crlb'], 'r-', linewidth=3, marker='s', label='CRLB')\n",
    "axes[0].set_xlabel('Sample Size')\n",
    "axes[0].set_ylabel('Variance')\n",
    "axes[0].set_title('Poisson MLE Variance vs CRLB')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Efficiency ratio\n",
    "axes[1].plot(poisson_results['n'], poisson_results['efficiency_ratio'], 'g-', linewidth=3, marker='D')\n",
    "axes[1].axhline(1.0, color='red', linestyle='--', alpha=0.7, label='Perfect Efficiency')\n",
    "axes[1].set_xlabel('Sample Size')\n",
    "axes[1].set_ylabel('Efficiency Ratio')\n",
    "axes[1].set_title('Poisson MLE Efficiency Ratio')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../slides/figures/poisson_crlb_achievement.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Poisson MLE asymptotic efficiency ratio: {poisson_results.iloc[-1]['efficiency_ratio']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exponential Rate: Asymptotic Efficiency\n",
    "\n",
    "Compare MLE and unbiased estimators for exponential rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_mle(x):\n",
    "    \"\"\"MLE for exponential rate: n / sum(x)\"\"\"\n",
    "    return len(x) / np.sum(x)\n",
    "\n",
    "def exponential_unbiased(x):\n",
    "    \"\"\"Unbiased estimator for exponential rate: (n-1) / sum(x)\"\"\"\n",
    "    n = len(x)\n",
    "    return (n - 1) / np.sum(x)\n",
    "\n",
    "def simulate_exponential_efficiency(true_lambda, n_values, R=5000):\n",
    "    \"\"\"\n",
    "    Compare MLE and unbiased estimators for exponential rate.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for n in n_values:\n",
    "        mle_estimates = np.zeros(R)\n",
    "        unbiased_estimates = np.zeros(R)\n",
    "\n",
    "        for r in range(R):\n",
    "            # Generate exponential with rate lambda (scale = 1/lambda)\n",
    "            sample = rng.exponential(1/true_lambda, n)\n",
    "            mle_estimates[r] = exponential_mle(sample)\n",
    "            unbiased_estimates[r] = exponential_unbiased(sample)\n",
    "\n",
    "        # MLE properties\n",
    "        mle_var = np.var(mle_estimates, ddof=0)\n",
    "        mle_bias = np.mean(mle_estimates) - true_lambda\n",
    "        mle_mse = np.mean((mle_estimates - true_lambda)**2)\n",
    "\n",
    "        # Unbiased properties\n",
    "        unbiased_var = np.var(unbiased_estimates, ddof=0)\n",
    "        unbiased_bias = np.mean(unbiased_estimates) - true_lambda\n",
    "        unbiased_mse = np.mean((unbiased_estimates - true_lambda)**2)\n",
    "\n",
    "        # CRLB\n",
    "        crlb = true_lambda**2 / n\n",
    "\n",
    "        results.append({\n",
    "            'n': n,\n",
    "            'mle_var': mle_var,\n",
    "            'mle_bias': mle_bias,\n",
    "            'mle_mse': mle_mse,\n",
    "            'unbiased_var': unbiased_var,\n",
    "            'unbiased_bias': unbiased_bias,\n",
    "            'unbiased_mse': unbiased_mse,\n",
    "            'crlb': crlb\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"Function defined: simulate_exponential_efficiency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate exponential efficiency\n",
    "true_lambda = 2.0\n",
    "exp_results = simulate_exponential_efficiency(true_lambda, n_values, R=10000)\n",
    "print(\"Exponential rate estimator comparison:\")\n",
    "print(exp_results.round(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot exponential comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Variance comparison\n",
    "axes[0,0].plot(exp_results['n'], exp_results['mle_var'], 'b-', linewidth=3, marker='o', label='MLE')\n",
    "axes[0,0].plot(exp_results['n'], exp_results['unbiased_var'], 'r-', linewidth=3, marker='s', label='Unbiased')\n",
    "axes[0,0].plot(exp_results['n'], exp_results['crlb'], 'g-', linewidth=3, marker='^', label='CRLB')\n",
    "axes[0,0].set_xlabel('Sample Size')\n",
    "axes[0,0].set_ylabel('Variance')\n",
    "axes[0,0].set_title('Exponential Estimator Variance')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Bias comparison\n",
    "axes[0,1].plot(exp_results['n'], exp_results['mle_bias'], 'b-', linewidth=3, marker='o', label='MLE')\n",
    "axes[0,1].plot(exp_results['n'], exp_results['unbiased_bias'], 'r-', linewidth=3, marker='s', label='Unbiased')\n",
    "axes[0,1].axhline(0, color='black', linestyle='--', alpha=0.7)\n",
    "axes[0,1].set_xlabel('Sample Size')\n",
    "axes[0,1].set_ylabel('Bias')\n",
    "axes[0,1].set_title('Exponential Estimator Bias')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# MSE comparison\n",
    "axes[1,0].plot(exp_results['n'], exp_results['mle_mse'], 'b-', linewidth=3, marker='o', label='MLE')\n",
    "axes[1,0].plot(exp_results['n'], exp_results['unbiased_mse'], 'r-', linewidth=3, marker='s', label='Unbiased')\n",
    "axes[1,0].set_xlabel('Sample Size')\n",
    "axes[1,0].set_ylabel('MSE')\n",
    "axes[1,0].set_title('Exponential Estimator MSE')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Efficiency ratios\n",
    "mle_efficiency = exp_results['mle_var'] / exp_results['crlb']\n",
    "unbiased_efficiency = exp_results['unbiased_var'] / exp_results['crlb']\n",
    "\n",
    "axes[1,1].plot(exp_results['n'], mle_efficiency, 'b-', linewidth=3, marker='o', label='MLE')\n",
    "axes[1,1].plot(exp_results['n'], unbiased_efficiency, 'r-', linewidth=3, marker='s', label='Unbiased')\n",
    "axes[1,1].axhline(1.0, color='black', linestyle='--', alpha=0.7, label='Perfect Efficiency')\n",
    "axes[1,1].set_xlabel('Sample Size')\n",
    "axes[1,1].set_ylabel('Efficiency Ratio')\n",
    "axes[1,1].set_title('Asymptotic Efficiency')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../slides/figures/exponential_efficiency.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"MLE asymptotic efficiency ratio: {mle_efficiency.iloc[-1]:.4f}\")\n",
    "print(f\"Unbiased asymptotic efficiency ratio: {unbiased_efficiency.iloc[-1]:.4f}\")\n",
    "print(\"MLE achieves CRLB asymptotically but has finite-sample bias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Confidence Interval Simulations\n",
    "\n",
    "These scenarios mirror the confidence-interval exercises. Each block reproduces the computations and visualisations directly in the notebook so they can be rerun or adapted.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise 1: $t$-Interval via the Pivotal Method\n",
    "\n",
    "We build a reusable helper for the one-sample $t$-interval and illustrate it on synthetic Normal data with unknown variance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TIntervalResult:\n",
    "    ci_lower: float\n",
    "    ci_upper: float\n",
    "    sample_mean: float\n",
    "    sample_sd: float\n",
    "    t_crit: float\n",
    "    n: int\n",
    "\n",
    "def t_interval(sample, confidence=0.95):\n",
    "    sample = np.asarray(sample, dtype=float)\n",
    "    n = sample.size\n",
    "    xbar = sample.mean()\n",
    "    s = sample.std(ddof=1)\n",
    "    alpha = 1.0 - confidence\n",
    "    t_crit = stats.t.ppf(1.0 - alpha / 2.0, df=n - 1)\n",
    "    margin = t_crit * s / np.sqrt(n)\n",
    "    return TIntervalResult(xbar - margin, xbar + margin, xbar, s, t_crit, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_data = rng.normal(loc=170.0, scale=10.0, size=20)\n",
    "t_result = t_interval(sample_data, confidence=0.95)\n",
    "print(f\"Sample size: n = {t_result.n}\")\n",
    "print(f\"Sample mean: x¯ = {t_result.sample_mean:.2f}\")\n",
    "print(f\"Sample sd: s = {t_result.sample_sd:.2f}\")\n",
    "print(f\"t critical value: {t_result.t_crit:.3f}\")\n",
    "print(f\"95% CI: [{t_result.ci_lower:.2f}, {t_result.ci_upper:.2f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise 2: Wilson vs Wald Interval\n",
    "\n",
    "For $\\hat p = 0.5$ we compare Wilson and Wald intervals over increasing sample sizes and observe their convergence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def wald_interval(p_hat, n, alpha=0.05):\n",
    "    z = stats.norm.ppf(1.0 - alpha / 2.0)\n",
    "    se = np.sqrt(p_hat * (1.0 - p_hat) / n)\n",
    "    return p_hat - z * se, p_hat + z * se\n",
    "\n",
    "def wilson_interval(x, n, alpha=0.05):\n",
    "    z = stats.norm.ppf(1.0 - alpha / 2.0)\n",
    "    z2 = z**2\n",
    "    p_tilde = (x + z2 / 2.0) / (n + z2)\n",
    "    se = np.sqrt(p_tilde * (1.0 - p_tilde) / (n + z2))\n",
    "    return p_tilde - z * se, p_tilde + z * se\n",
    "\n",
    "def compare_wald_wilson(sample_sizes, p_hat=0.5, alpha=0.05):\n",
    "    records = []\n",
    "    for n in sample_sizes:\n",
    "        x = int(round(p_hat * n))\n",
    "        wald_ci = wald_interval(p_hat, n, alpha=alpha)\n",
    "        wilson_ci = wilson_interval(x, n, alpha=alpha)\n",
    "        diff = max(abs(wald_ci[0] - wilson_ci[0]), abs(wald_ci[1] - wilson_ci[1]))\n",
    "        records.append({\n",
    "            'n': n,\n",
    "            'wald_lower': wald_ci[0],\n",
    "            'wald_upper': wald_ci[1],\n",
    "            'wilson_lower': wilson_ci[0],\n",
    "            'wilson_upper': wilson_ci[1],\n",
    "            'max_abs_diff': diff\n",
    "        })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "ci_comparison_df = compare_wald_wilson([10, 20, 50, 100, 500, 1000])\n",
    "ci_comparison_df.round(6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise 3: Proportion CIs on A/B Click Data\n",
    "\n",
    "We compute Wald, Wilson, Agresti–Coull, and Clopper–Pearson intervals for each variant and compare the click-through rates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def compute_cis_aggregated(impressions, clicks, confidence=0.95):\n",
    "    n = impressions\n",
    "    x = clicks\n",
    "    p_hat = x / n\n",
    "    alpha = 1.0 - confidence\n",
    "    z = stats.norm.ppf(1.0 - alpha / 2.0)\n",
    "\n",
    "    se_wald = np.sqrt(p_hat * (1.0 - p_hat) / n)\n",
    "    wald = (max(0.0, p_hat - z * se_wald), min(1.0, p_hat + z * se_wald))\n",
    "\n",
    "    z2 = z**2\n",
    "    p_tilde = (x + z2 / 2.0) / (n + z2)\n",
    "    se_wilson = np.sqrt(p_tilde * (1.0 - p_tilde) / (n + z2))\n",
    "    wilson = (p_tilde - z * se_wilson, p_tilde + z * se_wilson)\n",
    "\n",
    "    n_tilde = n + z2\n",
    "    x_tilde = x + z2 / 2.0\n",
    "    p_ac = x_tilde / n_tilde\n",
    "    se_ac = np.sqrt(p_ac * (1.0 - p_ac) / n_tilde)\n",
    "    agresti_coull = (p_ac - z * se_ac, p_ac + z * se_ac)\n",
    "\n",
    "    clopper_pearson = (\n",
    "        stats.beta.ppf(alpha / 2.0, x, n - x + 1) if x > 0 else 0.0,\n",
    "        stats.beta.ppf(1.0 - alpha / 2.0, x + 1, n - x) if x < n else 1.0\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'p_hat': p_hat,\n",
    "        'n': n,\n",
    "        'x': x,\n",
    "        'wald': wald,\n",
    "        'wilson': wilson,\n",
    "        'agresti_coull': agresti_coull,\n",
    "        'clopper_pearson': clopper_pearson\n",
    "    }\n",
    "\n",
    "ab_path = Path('../../shared/data/ab_test_clicks.csv')\n",
    "ab_df = pd.read_csv(ab_path)\n",
    "groups = ab_df.groupby('variant').agg({'impressions': 'sum', 'clicks': 'sum'}).reset_index()\n",
    "\n",
    "ab_results = {}\n",
    "for _, row in groups.iterrows():\n",
    "    ab_results[row['variant']] = compute_cis_aggregated(int(row['impressions']), int(row['clicks']))\n",
    "\n",
    "display_records = []\n",
    "for variant, res in ab_results.items():\n",
    "    for method in ['wald', 'wilson', 'agresti_coull', 'clopper_pearson']:\n",
    "        lower, upper = res[method]\n",
    "        display_records.append({\n",
    "            'variant': variant,\n",
    "            'method': method.replace('_', ' ').title(),\n",
    "            'lower': lower,\n",
    "            'upper': upper,\n",
    "            'width': upper - lower\n",
    "        })\n",
    "\n",
    "proportion_cis_df = pd.DataFrame(display_records)\n",
    "proportion_cis_df.round({'lower': 4, 'upper': 4, 'width': 4})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "method_labels = ['Wald', 'Wilson', 'Agresti-Coull', 'Clopper-Pearson']\n",
    "colors = ['firebrick', 'royalblue', 'seagreen', 'mediumpurple']\n",
    "\n",
    "for ax, (variant, res) in zip(axes, ab_results.items()):\n",
    "    for idx, method in enumerate(['wald', 'wilson', 'agresti_coull', 'clopper_pearson']):\n",
    "        lower, upper = res[method]\n",
    "        ax.plot([lower, upper], [idx, idx], marker='o', linewidth=2, color=colors[idx], label=method_labels[idx])\n",
    "    ax.axvline(res['p_hat'], color='black', linestyle='--', linewidth=2, label='p̂')\n",
    "    ax.set_yticks(range(len(method_labels)))\n",
    "    ax.set_yticklabels(method_labels)\n",
    "    ax.set_xlabel('Click-through rate')\n",
    "    ax.set_title(f\"{variant}: 95% CI\n",
    "(n={res['n']}, x={res['x']}, p̂={res['p_hat']:.4f})\")\n",
    "    ax.grid(True, axis='x', alpha=0.3)\n",
    "    ax.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "variant_keys = list(ab_results.keys())\n",
    "p_a, p_b = ab_results[variant_keys[0]]['p_hat'], ab_results[variant_keys[1]]['p_hat']\n",
    "n_a, n_b = ab_results[variant_keys[0]]['n'], ab_results[variant_keys[1]]['n']\n",
    "\n",
    "diff = p_a - p_b\n",
    "se_diff = np.sqrt(p_a * (1.0 - p_a) / n_a + p_b * (1.0 - p_b) / n_b)\n",
    "z = stats.norm.ppf(0.975)\n",
    "ci_diff = (diff - z * se_diff, diff + z * se_diff)\n",
    "print(f\"Difference in CTR ({variant_keys[0]} - {variant_keys[1]}): {diff:.4f}\")\n",
    "print(f\"95% CI for difference: [{ci_diff[0]:.4f}, {ci_diff[1]:.4f}]\")\n",
    "if ci_diff[0] > 0:\n",
    "    print('Conclusion: first variant significantly higher (α = 0.05)')\n",
    "elif ci_diff[1] < 0:\n",
    "    print('Conclusion: second variant significantly higher (α = 0.05)')\n",
    "else:\n",
    "    print('Conclusion: no significant difference (α = 0.05)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise 4: Delta Method CI for the Coefficient of Variation\n",
    "\n",
    "We approximate the standard error using the delta method and cross-check with a bootstrap.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cv_confidence_interval(sample, confidence=0.95):\n",
    "    sample = np.asarray(sample, dtype=float)\n",
    "    n = sample.size\n",
    "    xbar = sample.mean()\n",
    "    s = sample.std(ddof=1)\n",
    "    cv_hat = s / xbar\n",
    "    se_cv = cv_hat * np.sqrt((cv_hat**2 + 0.5) / n)\n",
    "    alpha = 1.0 - confidence\n",
    "    z = stats.norm.ppf(1.0 - alpha / 2.0)\n",
    "    ci = (cv_hat - z * se_cv, cv_hat + z * se_cv)\n",
    "    return cv_hat, se_cv, ci\n",
    "\n",
    "height_sample = rng.normal(loc=170.0, scale=10.0, size=100)\n",
    "cv_hat, se_cv, ci_delta = cv_confidence_interval(height_sample, confidence=0.95)\n",
    "\n",
    "boot_draws = 5_000\n",
    "bootstrap_cv = np.empty(boot_draws)\n",
    "for b in range(boot_draws):\n",
    "    bootstrap_sample = rng.choice(height_sample, size=height_sample.size, replace=True)\n",
    "    bootstrap_cv[b] = bootstrap_sample.std(ddof=1) / bootstrap_sample.mean()\n",
    "\n",
    "ci_boot = np.percentile(bootstrap_cv, [2.5, 97.5])\n",
    "\n",
    "print('Coefficient of variation analysis')\n",
    "print('=' * 40)\n",
    "print(f'Sample size: n = {height_sample.size}')\n",
    "print(f'Sample mean: {height_sample.mean():.2f}')\n",
    "print(f'Sample sd: {height_sample.std(ddof=1):.2f}')\n",
    "print(f'CV estimate: {cv_hat:.4f}')\n",
    "print(f'Delta-method SE: {se_cv:.4f}')\n",
    "print(f'Delta-method 95% CI: [{ci_delta[0]:.4f}, {ci_delta[1]:.4f}]')\n",
    "print(f'Bootstrap 95% CI: [{ci_boot[0]:.4f}, {ci_boot[1]:.4f}]')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "axes[0].errorbar([0], [cv_hat], yerr=[[cv_hat - ci_delta[0]], [ci_delta[1] - cv_hat]], fmt='o', capsize=10, linewidth=2)\n",
    "axes[0].set_xticks([0])\n",
    "axes[0].set_xticklabels(['CV'])\n",
    "axes[0].set_ylabel('Coefficient of variation')\n",
    "axes[0].set_title('Delta-method interval')\n",
    "axes[0].grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "axes[1].hist(bootstrap_cv, bins=40, density=True, alpha=0.7, edgecolor='black')\n",
    "axes[1].axvline(cv_hat, color='red', linestyle='--', linewidth=2, label='Estimate')\n",
    "axes[1].axvline(ci_boot[0], color='blue', linestyle='--', linewidth=2, label='Bootstrap CI')\n",
    "axes[1].axvline(ci_boot[1], color='blue', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Coefficient of variation')\n",
    "axes[1].set_ylabel('Density')\n",
    "axes[1].set_title('Bootstrap distribution')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary and Key Takeaways\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. Fisher information quantifies the information content of data\n",
    "2. CRLB sets theoretical minimum variance for unbiased estimators\n",
    "3. Sample mean achieves CRLB for Normal and Poisson models\n",
    "4. MLE achieves CRLB asymptotically for exponential rate\n",
    "5. Tradeoffs between bias and efficiency in finite samples\n",
    "\n",
    "Key insights:\n",
    "- Fisher information I(θ) = Var(U(θ)) measures precision\n",
    "- CRLB = 1/I_n(θ) is the theoretical efficiency bound\n",
    "- MLEs achieve CRLB asymptotically under regularity conditions\n",
    "- Lesson 2 estimators are asymptotically efficient\n",
    "- Finite-sample properties may differ from asymptotic behavior"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "applied-stats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
