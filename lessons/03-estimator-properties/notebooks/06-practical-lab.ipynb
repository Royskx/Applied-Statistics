{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98cbac00",
   "metadata": {},
   "source": [
    "# Lesson 3: Practical Lab Session\n",
    "\n",
    "**Estimator Properties - Hands-On Exercises**\n",
    "\n",
    "This notebook contains practical exercises for exploring:\n",
    "- Bias-variance tradeoff through simulation\n",
    "- Confidence interval coverage properties\n",
    "- Bootstrap methods and diagnostics\n",
    "- Real-world A/B testing analysis\n",
    "\n",
    "**Prerequisites**: Complete notebooks 01-05 for theoretical background\n",
    "\n",
    "**Estimated time**: 2-3 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd9b710",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import required libraries and configure plotting style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ff4eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "rng = np.random.default_rng(2025)\n",
    "\n",
    "# Configure plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"colorblind\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"✓ Setup complete\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "# print(f\"SciPy version: {stats.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed94e70c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lab Task 1: Bias-Variance Tradeoff Investigation\n",
    "\n",
    "**Goal**: Compare MLE vs. unbiased estimator for variance\n",
    "\n",
    "**Theory**: \n",
    "- MLE: $\\hat{\\sigma}^2_{\\text{MLE}} = \\frac{1}{n}\\sum(X_i - \\bar{X})^2$ (biased)\n",
    "- Unbiased: $s^2 = \\frac{1}{n-1}\\sum(X_i - \\bar{X})^2$ (unbiased)\n",
    "- MSE = Bias² + Variance\n",
    "\n",
    "**Question**: Which estimator has lower MSE for small samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6451c1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_variance_estimators(true_mu=0, true_sigma=2, n=20, n_sim=10000):\n",
    "    \"\"\"\n",
    "    Simulate and compare variance estimators\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    true_mu : float\n",
    "        True population mean\n",
    "    true_sigma : float\n",
    "        True population standard deviation\n",
    "    n : int\n",
    "        Sample size\n",
    "    n_sim : int\n",
    "        Number of simulations\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Results containing bias, variance, and MSE for each estimator\n",
    "    \"\"\"\n",
    "    mle_estimates = []\n",
    "    unbiased_estimates = []\n",
    "\n",
    "    for _ in range(n_sim):\n",
    "        # Generate sample\n",
    "        X = rng.normal(true_mu, true_sigma, n)\n",
    "        xbar = X.mean()\n",
    "\n",
    "        # MLE estimator\n",
    "        mle_var = np.sum((X - xbar)**2) / n\n",
    "        mle_estimates.append(mle_var)\n",
    "\n",
    "        # Unbiased estimator\n",
    "        unbiased_var = np.sum((X - xbar)**2) / (n - 1)\n",
    "        unbiased_estimates.append(unbiased_var)\n",
    "\n",
    "    # Convert to arrays\n",
    "    mle_arr = np.array(mle_estimates)\n",
    "    unb_arr = np.array(unbiased_estimates)\n",
    "\n",
    "    # Compute metrics\n",
    "    true_var = true_sigma**2\n",
    "\n",
    "    results = {\n",
    "        'MLE': {\n",
    "            'bias': mle_arr.mean() - true_var,\n",
    "            'variance': mle_arr.var(),\n",
    "            'mse': ((mle_arr - true_var)**2).mean()\n",
    "        },\n",
    "        'Unbiased': {\n",
    "            'bias': unb_arr.mean() - true_var,\n",
    "            'variance': unb_arr.var(),\n",
    "            'mse': ((unb_arr - true_var)**2).mean()\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Histogram\n",
    "    axes[0].hist(mle_arr, bins=50, alpha=0.6, label='MLE', density=True)\n",
    "    axes[0].hist(unb_arr, bins=50, alpha=0.6, label='Unbiased', density=True)\n",
    "    axes[0].axvline(true_var, color='red', linestyle='--', linewidth=2, label='True σ²')\n",
    "    axes[0].set_xlabel('Estimated Variance')\n",
    "    axes[0].set_ylabel('Density')\n",
    "    axes[0].set_title(f'Distribution of Variance Estimators (n={n})')\n",
    "    axes[0].legend()\n",
    "\n",
    "    # MSE comparison\n",
    "    metrics = ['Bias²', 'Variance', 'MSE']\n",
    "    mle_vals = [results['MLE']['bias']**2, results['MLE']['variance'], results['MLE']['mse']]\n",
    "    unb_vals = [results['Unbiased']['bias']**2, results['Unbiased']['variance'], results['Unbiased']['mse']]\n",
    "\n",
    "    x_pos = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "\n",
    "    axes[1].bar(x_pos - width/2, mle_vals, width, label='MLE', alpha=0.8)\n",
    "    axes[1].bar(x_pos + width/2, unb_vals, width, label='Unbiased', alpha=0.8)\n",
    "    axes[1].set_xticks(x_pos)\n",
    "    axes[1].set_xticklabels(metrics)\n",
    "    axes[1].set_ylabel('Value')\n",
    "    axes[1].set_title('Estimator Comparison')\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dd9528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulation\n",
    "print(\"Running variance estimator simulation (n=20, 10,000 iterations)...\\n\")\n",
    "results = simulate_variance_estimators(true_mu=0, true_sigma=2, n=20, n_sim=10000)\n",
    "\n",
    "print(\"\\nSimulation Results:\")\n",
    "print(\"=\"*50)\n",
    "for name, metrics in results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric:10s}: {value:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Key Observation:\")\n",
    "if results['MLE']['mse'] < results['Unbiased']['mse']:\n",
    "    print(\"MLE has lower MSE despite being biased!\")\n",
    "    print(\"This demonstrates the bias-variance tradeoff.\")\n",
    "else:\n",
    "    print(\"Unbiased estimator has lower MSE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9af69c1",
   "metadata": {},
   "source": [
    "**Exercise**: Try different sample sizes (n=10, 50, 100) and observe how the tradeoff changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b06855d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: Test with different sample sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4606504",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lab Task 2: Confidence Interval Coverage Study\n",
    "\n",
    "**Goal**: Compare Wald, Wilson, and Agresti-Coull intervals for proportions\n",
    "\n",
    "**Theory**:\n",
    "- **Nominal coverage**: The claimed confidence level (e.g., 95%)\n",
    "- **Actual coverage**: Proportion of CIs that contain true parameter\n",
    "- Good methods have actual ≈ nominal coverage\n",
    "\n",
    "**Question**: Which method maintains proper coverage across different p and n?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa24de51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coverage_study(n_sim=5000, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Compare coverage of different CI methods for proportions\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_sim : int\n",
    "        Number of simulations per (p, n) combination\n",
    "    alpha : float\n",
    "        Significance level\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Coverage results for each method\n",
    "    \"\"\"\n",
    "    p_values = [0.05, 0.1, 0.3, 0.5]\n",
    "    n_values = [20, 50, 100, 200]\n",
    "    methods = ['Wald', 'Wilson', 'Agresti-Coull']\n",
    "\n",
    "    results = {method: np.zeros((len(p_values), len(n_values)))\n",
    "               for method in methods}\n",
    "\n",
    "    z = stats.norm.ppf(1 - alpha/2)  # ~1.96 for 95% CI\n",
    "\n",
    "    for i, p in enumerate(p_values):\n",
    "        for j, n in enumerate(n_values):\n",
    "            coverage = {method: 0 for method in methods}\n",
    "\n",
    "            for _ in range(n_sim):\n",
    "                # Generate data\n",
    "                X = rng.binomial(1, p, size=n)\n",
    "                phat = X.mean()\n",
    "\n",
    "                # Wald interval\n",
    "                se_wald = np.sqrt(phat * (1 - phat) / n)\n",
    "                wald_lo = phat - z * se_wald\n",
    "                wald_hi = phat + z * se_wald\n",
    "                coverage['Wald'] += (wald_lo <= p <= wald_hi)\n",
    "\n",
    "                # Wilson interval\n",
    "                denom = 1 + z**2 / n\n",
    "                center = (phat + z**2 / (2*n)) / denom\n",
    "                radius = z * np.sqrt(phat*(1-phat)/n + z**2/(4*n**2)) / denom\n",
    "                wilson_lo = center - radius\n",
    "                wilson_hi = center + radius\n",
    "                coverage['Wilson'] += (wilson_lo <= p <= wilson_hi)\n",
    "\n",
    "                # Agresti-Coull interval\n",
    "                n_tilde = n + 4\n",
    "                p_tilde = (X.sum() + 2) / n_tilde\n",
    "                se_ac = np.sqrt(p_tilde * (1 - p_tilde) / n_tilde)\n",
    "                ac_lo = p_tilde - z * se_ac\n",
    "                ac_hi = p_tilde + z * se_ac\n",
    "                coverage['Agresti-Coull'] += (ac_lo <= p <= ac_hi)\n",
    "\n",
    "            # Store coverage proportions\n",
    "            for method in methods:\n",
    "                results[method][i, j] = coverage[method] / n_sim\n",
    "\n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    for ax, method in zip(axes, methods):\n",
    "        im = ax.imshow(results[method], cmap='RdYlGn', vmin=0.90, vmax=0.99, aspect='auto')\n",
    "        ax.set_xticks(range(len(n_values)))\n",
    "        ax.set_xticklabels(n_values)\n",
    "        ax.set_yticks(range(len(p_values)))\n",
    "        ax.set_yticklabels(p_values)\n",
    "        ax.set_xlabel('Sample Size (n)')\n",
    "        ax.set_ylabel('True Proportion (p)')\n",
    "        ax.set_title(f'{method} Method\\n({int((1-alpha)*100)}% nominal coverage)')\n",
    "\n",
    "        # Add text annotations\n",
    "        for i in range(len(p_values)):\n",
    "            for j in range(len(n_values)):\n",
    "                text = ax.text(j, i, f'{results[method][i, j]:.3f}',\n",
    "                             ha=\"center\", va=\"center\", color=\"black\", fontsize=9)\n",
    "\n",
    "        plt.colorbar(im, ax=ax, label='Coverage Probability')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66785b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run coverage study (this may take 1-2 minutes)\n",
    "print(\"Running CI coverage study...\")\n",
    "print(\"Testing 4 proportions × 4 sample sizes × 5000 simulations = 80,000 datasets\")\n",
    "print(\"This will take approximately 1-2 minutes...\\n\")\n",
    "\n",
    "coverage_results = coverage_study(n_sim=5000)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Key Findings:\")\n",
    "print(\"=\"*70)\n",
    "print(\"1. Wald interval: Poor coverage, especially for extreme p and small n\")\n",
    "print(\"2. Wilson interval: Maintains near-nominal coverage across all scenarios\")\n",
    "print(\"3. Agresti-Coull: Good approximation to Wilson, slightly conservative\")\n",
    "print(\"\\nRecommendation: Use Wilson or Agresti-Coull, NOT Wald!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824598de",
   "metadata": {},
   "source": [
    "**Exercise**: What happens with p=0.01 and n=20? Try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb5ee5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run coverage study (this may take 1-2 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b73f88a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lab Task 3: Bootstrap Confidence Intervals with Diagnostics\n",
    "\n",
    "**Goal**: Apply bootstrap to estimate median and compare with parametric methods\n",
    "\n",
    "**Theory**:\n",
    "- Bootstrap resamples data with replacement\n",
    "- Approximates sampling distribution of any statistic\n",
    "- Useful when theoretical distribution is unknown or complex\n",
    "\n",
    "**Question**: How well does bootstrap work for the median?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fdbe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_analysis(data, statistic=np.median, n_bootstrap=10000, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Comprehensive bootstrap analysis with diagnostics\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array-like\n",
    "        Original sample data\n",
    "    statistic : callable\n",
    "        Function to compute on each bootstrap sample\n",
    "    n_bootstrap : int\n",
    "        Number of bootstrap resamples\n",
    "    alpha : float\n",
    "        Significance level\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Bootstrap results including CIs and diagnostics\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "\n",
    "    # Original statistic\n",
    "    theta_hat = statistic(data)\n",
    "\n",
    "    # Generate bootstrap samples\n",
    "    bootstrap_stats = np.array([\n",
    "        statistic(rng.choice(data, size=n, replace=True))\n",
    "        for _ in range(n_bootstrap)\n",
    "    ])\n",
    "\n",
    "    # Bootstrap SE\n",
    "    bootstrap_se = bootstrap_stats.std()\n",
    "\n",
    "    # Different CI methods\n",
    "    # 1. Percentile\n",
    "    ci_percentile = np.percentile(bootstrap_stats, [100*alpha/2, 100*(1-alpha/2)])\n",
    "\n",
    "    # 2. Normal approximation\n",
    "    ci_normal = (theta_hat - stats.norm.ppf(1-alpha/2) * bootstrap_se,\n",
    "                 theta_hat + stats.norm.ppf(1-alpha/2) * bootstrap_se)\n",
    "\n",
    "    # 3. Basic bootstrap\n",
    "    ci_basic = (2*theta_hat - np.percentile(bootstrap_stats, 100*(1-alpha/2)),\n",
    "                2*theta_hat - np.percentile(bootstrap_stats, 100*alpha/2))\n",
    "\n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    # 1. Bootstrap distribution\n",
    "    axes[0, 0].hist(bootstrap_stats, bins=50, density=True, alpha=0.7, edgecolor='black')\n",
    "    axes[0, 0].axvline(theta_hat, color='red', linestyle='--', linewidth=2, label='Original')\n",
    "    axes[0, 0].axvline(ci_percentile[0], color='green', linestyle='--', label='95% CI')\n",
    "    axes[0, 0].axvline(ci_percentile[1], color='green', linestyle='--')\n",
    "    axes[0, 0].set_xlabel('Bootstrap Statistic')\n",
    "    axes[0, 0].set_ylabel('Density')\n",
    "    axes[0, 0].set_title('Bootstrap Distribution')\n",
    "    axes[0, 0].legend()\n",
    "\n",
    "    # 2. Q-Q plot\n",
    "    stats.probplot(bootstrap_stats, dist=\"norm\", plot=axes[0, 1])\n",
    "    axes[0, 1].set_title('Q-Q Plot vs. Normal')\n",
    "\n",
    "    # 3. Convergence plot\n",
    "    cumulative_means = np.cumsum(bootstrap_stats) / np.arange(1, n_bootstrap + 1)\n",
    "    axes[1, 0].plot(cumulative_means, linewidth=0.8)\n",
    "    axes[1, 0].axhline(theta_hat, color='red', linestyle='--', label='Original')\n",
    "    axes[1, 0].set_xlabel('Number of Bootstrap Samples')\n",
    "    axes[1, 0].set_ylabel('Cumulative Mean')\n",
    "    axes[1, 0].set_title('Bootstrap Convergence')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].set_xlim([0, n_bootstrap])\n",
    "\n",
    "    # 4. CI comparison\n",
    "    methods = ['Percentile', 'Normal', 'Basic']\n",
    "    cis = [ci_percentile, ci_normal, ci_basic]\n",
    "\n",
    "    for i, (method, ci) in enumerate(zip(methods, cis)):\n",
    "        axes[1, 1].plot([ci[0], ci[1]], [i, i], 'o-', linewidth=2, markersize=8, label=method)\n",
    "\n",
    "    axes[1, 1].axvline(theta_hat, color='red', linestyle='--', alpha=0.5, label='Point Estimate')\n",
    "    axes[1, 1].set_yticks(range(len(methods)))\n",
    "    axes[1, 1].set_yticklabels(methods)\n",
    "    axes[1, 1].set_xlabel('Value')\n",
    "    axes[1, 1].set_title('CI Methods Comparison')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Original statistic: {theta_hat:.4f}\")\n",
    "    print(f\"Bootstrap SE: {bootstrap_se:.4f}\")\n",
    "    print(f\"\\n95% Confidence Intervals:\")\n",
    "    print(f\"  Percentile:    [{ci_percentile[0]:.4f}, {ci_percentile[1]:.4f}]\")\n",
    "    print(f\"  Normal approx: [{ci_normal[0]:.4f}, {ci_normal[1]:.4f}]\")\n",
    "    print(f\"  Basic:         [{ci_basic[0]:.4f}, {ci_basic[1]:.4f}]\")\n",
    "\n",
    "    return {\n",
    "        'statistic': theta_hat,\n",
    "        'bootstrap_se': bootstrap_se,\n",
    "        'ci_percentile': ci_percentile,\n",
    "        'ci_normal': ci_normal,\n",
    "        'ci_basic': ci_basic,\n",
    "        'bootstrap_distribution': bootstrap_stats\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec1c09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load height data\n",
    "print(\"Loading height/weight data...\")\n",
    "heights_df = pd.read_csv(\"../../../shared/data/heights_weights_sample.csv\")\n",
    "heights = heights_df['height_cm'].values\n",
    "\n",
    "print(f\"Dataset: {len(heights)} observations\")\n",
    "print(f\"Mean height: {heights.mean():.2f}\")\n",
    "print(f\"Median height: {np.median(heights):.2f}\")\n",
    "print(f\"Std dev: {heights.std():.2f}\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb6e8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap analysis for median height\n",
    "print(\"\\n=== Bootstrap Analysis for Median Height ===\")\n",
    "print(\"Running 10,000 bootstrap resamples...\\n\")\n",
    "\n",
    "results_median = bootstrap_analysis(heights, statistic=np.median, n_bootstrap=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce40bf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap analysis for IQR\n",
    "def iqr(x):\n",
    "    \"\"\"Interquartile range\"\"\"\n",
    "    return np.percentile(x, 75) - np.percentile(x, 25)\n",
    "\n",
    "print(\"\\n\\n=== Bootstrap Analysis for IQR ===\")\n",
    "print(\"Running 10,000 bootstrap resamples...\\n\")\n",
    "\n",
    "results_iqr = bootstrap_analysis(heights, statistic=iqr, n_bootstrap=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4ae2d3",
   "metadata": {},
   "source": [
    "**Exercise**: Try bootstrapping other statistics like:\n",
    "- 90th percentile\n",
    "- Coefficient of variation (std/mean)\n",
    "- Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f084bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: Try other statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f25c4e2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lab Task 4: Real Data Application - A/B Testing\n",
    "\n",
    "**Goal**: Analyze A/B test data using multiple inference methods\n",
    "\n",
    "**Scenario**: Two website designs (variants A and B) tested for click-through rates\n",
    "\n",
    "**Data format**: The CSV contains aggregated data per user:\n",
    "- `user_id`: Unique user identifier\n",
    "- `variant`: Which design the user saw ('A' or 'B')\n",
    "- `impressions`: Number of times the user saw the design\n",
    "- `clicks`: Number of times the user clicked\n",
    "\n",
    "**Question**: Is Design B significantly better than Design A?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680311ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load A/B test data\n",
    "print(\"Loading A/B test data...\\n\")\n",
    "ab_data = pd.read_csv(\"../../../shared/data/ab_test_clicks.csv\")\n",
    "\n",
    "print(\"Data preview:\")\n",
    "print(ab_data.head(10))\n",
    "print(f\"\\nData shape: {ab_data.shape}\")\n",
    "print(f\"Columns: {list(ab_data.columns)}\")\n",
    "print(f\"\\nVariants: {ab_data['variant'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba7e797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics from aggregated data\n",
    "# Group A\n",
    "group_A = ab_data[ab_data['variant'] == 'A']\n",
    "n_A = group_A['impressions'].sum()  # Total impressions for A\n",
    "clicks_A = group_A['clicks'].sum()   # Total clicks for A\n",
    "\n",
    "# Group B\n",
    "group_B = ab_data[ab_data['variant'] == 'B']\n",
    "n_B = group_B['impressions'].sum()  # Total impressions for B\n",
    "clicks_B = group_B['clicks'].sum()   # Total clicks for B\n",
    "\n",
    "# Click-through rates\n",
    "p_A = clicks_A / n_A\n",
    "p_B = clicks_B / n_B\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Summary Statistics:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Group A: {clicks_A}/{n_A} impressions clicked (p̂ = {p_A:.4f})\")\n",
    "print(f\"Group B: {clicks_B}/{n_B} impressions clicked (p̂ = {p_B:.4f})\")\n",
    "print(f\"Difference (B - A): {p_B - p_A:.4f}\")\n",
    "if p_A > 0:\n",
    "    print(f\"Relative lift: {((p_B - p_A) / p_A * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2a5298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Wald CI for difference\n",
    "def proportion_diff_ci_wald(n1, p1, n2, p2, alpha=0.05):\n",
    "    \"\"\"Wald CI for difference in proportions\"\"\"\n",
    "    diff = p2 - p1\n",
    "    se = np.sqrt(p1*(1-p1)/n1 + p2*(1-p2)/n2)\n",
    "    z = stats.norm.ppf(1 - alpha/2)\n",
    "    return (diff - z*se, diff + z*se)\n",
    "\n",
    "ci_wald = proportion_diff_ci_wald(n_A, p_A, n_B, p_B)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Method 1: Wald Confidence Interval\")\n",
    "print(\"=\"*70)\n",
    "print(f\"95% CI for difference (B - A): [{ci_wald[0]:.4f}, {ci_wald[1]:.4f}]\")\n",
    "if ci_wald[0] > 0:\n",
    "    print(\"✓ Difference is statistically significant (95% confidence)\")\n",
    "    print(\"  → Group B has higher click-through rate\")\n",
    "elif ci_wald[1] < 0:\n",
    "    print(\"✓ Difference is statistically significant (95% confidence)\")\n",
    "    print(\"  → Group A has higher click-through rate\")\n",
    "else:\n",
    "    print(\"✗ No significant difference detected at 95% confidence level\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee92781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Bootstrap CI for difference\n",
    "def bootstrap_diff_proportions_aggregated(group_A_df, group_B_df, n_bootstrap=10000, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Bootstrap CI for difference in proportions from aggregated data\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    group_A_df, group_B_df : DataFrames with 'impressions' and 'clicks' columns\n",
    "    n_bootstrap : int\n",
    "        Number of bootstrap resamples\n",
    "    alpha : float\n",
    "        Significance level\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (CI, bootstrap differences)\n",
    "    \"\"\"\n",
    "    # Expand aggregated data to individual binary outcomes\n",
    "    def expand_group(df):\n",
    "        outcomes = []\n",
    "        for _, row in df.iterrows():\n",
    "            # Add clicks (1s)\n",
    "            outcomes.extend([1] * row['clicks'])\n",
    "            # Add non-clicks (0s)\n",
    "            outcomes.extend([0] * (row['impressions'] - row['clicks']))\n",
    "        return np.array(outcomes)\n",
    "\n",
    "    data_A = expand_group(group_A_df)\n",
    "    data_B = expand_group(group_B_df)\n",
    "\n",
    "    # Bootstrap resampling\n",
    "    diffs = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        boot_A = rng.choice(data_A, size=len(data_A), replace=True)\n",
    "        boot_B = rng.choice(data_B, size=len(data_B), replace=True)\n",
    "        diffs.append(boot_B.mean() - boot_A.mean())\n",
    "\n",
    "    diffs = np.array(diffs)\n",
    "    ci = np.percentile(diffs, [100*alpha/2, 100*(1-alpha/2)])\n",
    "    return ci, diffs\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Method 2: Bootstrap Confidence Interval\")\n",
    "print(\"=\"*70)\n",
    "print(\"Running 10,000 bootstrap resamples...\")\n",
    "\n",
    "ci_bootstrap, boot_diffs = bootstrap_diff_proportions_aggregated(group_A, group_B)\n",
    "\n",
    "print(f\"95% CI for difference (B - A): [{ci_bootstrap[0]:.4f}, {ci_bootstrap[1]:.4f}]\")\n",
    "if ci_bootstrap[0] > 0:\n",
    "    print(\"✓ Difference is statistically significant (95% confidence)\")\n",
    "    print(\"  → Group B has higher click-through rate\")\n",
    "elif ci_bootstrap[1] < 0:\n",
    "    print(\"✓ Difference is statistically significant (95% confidence)\")\n",
    "    print(\"  → Group A has higher click-through rate\")\n",
    "else:\n",
    "    print(\"✗ No significant difference detected at 95% confidence level\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaddee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Bootstrap distribution\n",
    "axes[0].hist(boot_diffs, bins=50, density=True, alpha=0.7, edgecolor='black')\n",
    "axes[0].axvline(p_B - p_A, color='red', linestyle='--', linewidth=2, label='Observed Difference')\n",
    "axes[0].axvline(0, color='gray', linestyle=':', linewidth=1, label='No Difference')\n",
    "axes[0].axvline(ci_bootstrap[0], color='green', linestyle='--', label='95% Bootstrap CI')\n",
    "axes[0].axvline(ci_bootstrap[1], color='green', linestyle='--')\n",
    "axes[0].set_xlabel('Difference in Proportions (B - A)')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].set_title('Bootstrap Distribution of Difference')\n",
    "axes[0].legend()\n",
    "\n",
    "# Right: CI comparison\n",
    "methods = ['Wald', 'Bootstrap']\n",
    "cis = [ci_wald, ci_bootstrap]\n",
    "\n",
    "for i, (method, ci) in enumerate(zip(methods, cis)):\n",
    "    axes[1].plot([ci[0], ci[1]], [i, i], 'o-', linewidth=2, markersize=8, label=method)\n",
    "\n",
    "axes[1].axvline(p_B - p_A, color='red', linestyle='--', alpha=0.5, label='Observed')\n",
    "axes[1].axvline(0, color='gray', linestyle=':', linewidth=1, label='No Difference')\n",
    "axes[1].set_yticks(range(len(methods)))\n",
    "axes[1].set_yticklabels(methods)\n",
    "axes[1].set_xlabel('Difference in Proportions (B - A)')\n",
    "axes[1].set_title('Confidence Intervals Comparison')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5444b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical test: two-proportion z-test\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "counts = np.array([clicks_B, clicks_A])\n",
    "nobs = np.array([n_B, n_A])\n",
    "\n",
    "z_stat, p_value = proportions_ztest(counts, nobs)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Method 3: Two-Proportion Z-Test\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Z-statistic: {z_stat:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(f\"✓ Reject null hypothesis at α=0.05 (p={p_value:.4f} < 0.05)\")\n",
    "    print(\"  → Significant difference between groups\")\n",
    "else:\n",
    "    print(f\"✗ Fail to reject null hypothesis at α=0.05 (p={p_value:.4f} ≥ 0.05)\")\n",
    "    print(\"  → No significant difference detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5603455c",
   "metadata": {},
   "source": [
    "### Final Recommendation\n",
    "\n",
    "Based on the analysis using three different methods:\n",
    "1. **Wald CI**: Traditional approach\n",
    "2. **Bootstrap CI**: Distribution-free approach\n",
    "3. **Z-test**: Hypothesis test\n",
    "\n",
    "All methods should give consistent conclusions about whether the difference is statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9606d4fb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Conclusions\n",
    "\n",
    "In this lab, you have:\n",
    "\n",
    "1. ✅ **Explored the bias-variance tradeoff** through simulation\n",
    "   - Observed that MLE can have lower MSE despite being biased\n",
    "   - Learned that the \"best\" estimator depends on the metric (bias vs. MSE)\n",
    "\n",
    "2. ✅ **Compared CI coverage properties** for proportions\n",
    "   - Discovered that Wald intervals have poor coverage\n",
    "   - Learned that Wilson and Agresti-Coull methods are superior\n",
    "\n",
    "3. ✅ **Applied bootstrap methods** with comprehensive diagnostics\n",
    "   - Constructed CIs for median and IQR using bootstrap\n",
    "   - Assessed bootstrap distribution quality with Q-Q plots\n",
    "   - Verified bootstrap convergence\n",
    "\n",
    "4. ✅ **Analyzed real A/B test data** using multiple methods\n",
    "   - Compared Wald, bootstrap, and hypothesis testing approaches\n",
    "   - Drew conclusions about statistical significance\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **MSE is often more important than bias alone** when evaluating estimators\n",
    "- **Not all CI methods are created equal** - coverage matters more than simplicity\n",
    "- **Bootstrap is powerful but requires sufficient sample size** (n ≥ 30 typically)\n",
    "- **Multiple inference methods should give consistent results** when assumptions are met\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Review theoretical notebooks (01-05) for deeper understanding\n",
    "- Complete exercises in material.md\n",
    "- Apply these methods to your own datasets\n",
    "- Proceed to Lesson 4: Hypothesis Testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "applied-stats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
