{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3 Module 2: Consistency of Estimators\n",
    "\n",
    "This notebook demonstrates consistency concepts using practical examples.\n",
    "It builds on Lesson 1 (LLN/CLT) and connects to Lesson 2 (MLE/MoM estimators).\n",
    "\n",
    "## Learning Objectives\n",
    "- Define consistency (convergence in probability) and strong consistency\n",
    "- Connect consistency to the Law of Large Numbers (Lesson 1)\n",
    "- Identify consistent vs inconsistent estimators\n",
    "- Apply consistency concepts to MLE and MoM estimators (Lesson 2)\n",
    "\n",
    "## Repository Context\n",
    "- Uses the `uniform_max_consistency()` function from the appendix\n",
    "- Demonstrates connections to Lesson 1 (LLN/CLT) foundations\n",
    "- Shows consistency of Lesson 2 estimators (MLE/MoM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Set style and random seed\n",
    "sns.set_theme(context=\"talk\", style=\"whitegrid\")\n",
    "sns.set_palette([\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\",\n",
    "                 \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\"])\n",
    "rng = np.random.default_rng(2025)\n",
    "\n",
    "print(\"Environment setup complete. Random seed: 2025\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Consistency Demonstration Functions\n",
    "\n",
    "Let's implement functions to demonstrate consistency empirically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_consistency(estimator_fn, true_theta, gen_fn, n_values, R=1000, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Empirically check consistency by computing convergence rates.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    estimator_fn : callable\n",
    "        Function that takes sample and returns estimate\n",
    "    true_theta : float\n",
    "        True parameter value\n",
    "    gen_fn : callable\n",
    "        Function that generates sample of size n\n",
    "    n_values : array-like\n",
    "        Sample sizes to test\n",
    "    R : int\n",
    "        Number of replications per sample size\n",
    "    epsilon : float\n",
    "        Tolerance for |estimator - true| > epsilon\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with sample sizes and convergence rates\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for n in n_values:\n",
    "        errors = np.zeros(R)\n",
    "\n",
    "        for r in range(R):\n",
    "            sample = gen_fn(n)\n",
    "            estimate = estimator_fn(sample)\n",
    "            errors[r] = abs(estimate - true_theta)\n",
    "\n",
    "        # Proportion of estimates within epsilon of truth\n",
    "        convergence_rate = np.mean(errors <= epsilon)\n",
    "\n",
    "        results.append({\n",
    "            'n': n,\n",
    "            'convergence_rate': convergence_rate,\n",
    "            'mean_error': np.mean(errors),\n",
    "            'std_error': np.std(errors, ddof=1)\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"Function defined: check_consistency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sample Mean Consistency (Normal Data)\n",
    "\n",
    "Demonstrate that sample means are consistent for population means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_mean(x):\n",
    "    return np.mean(x)\n",
    "\n",
    "def generate_normal(n):\n",
    "    return rng.normal(5.0, 2.0, n)  # mu=5, sigma=2\n",
    "\n",
    "# Test consistency across sample sizes\n",
    "n_values = [5, 10, 20, 50, 100, 200, 500]\n",
    "consistency_results = check_consistency(\n",
    "    sample_mean, 5.0, generate_normal, n_values, R=2000, epsilon=0.1\n",
    ")\n",
    "\n",
    "print(\"Sample mean consistency results:\")\n",
    "print(consistency_results.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot convergence\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Convergence rate\n",
    "axes[0].plot(consistency_results['n'], consistency_results['convergence_rate'], 'b-', linewidth=3, marker='o')\n",
    "axes[0].axhline(0.95, color='red', linestyle='--', alpha=0.7, label='95% target')\n",
    "axes[0].set_xlabel('Sample Size')\n",
    "axes[0].set_ylabel('Convergence Rate')\n",
    "axes[0].set_title('Sample Mean Consistency\\nP(|X̄ₙ - μ| ≤ 0.1)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Mean error vs sample size\n",
    "axes[1].plot(consistency_results['n'], consistency_results['mean_error'], 'r-', linewidth=3, marker='o')\n",
    "axes[1].set_xlabel('Sample Size')\n",
    "axes[1].set_ylabel('Mean Absolute Error')\n",
    "axes[1].set_title('Error Decreases with Sample Size')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../slides/figures/sample_mean_consistency.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"For n={n_values[-1]}: Convergence rate = {consistency_results.iloc[-1]['convergence_rate']:.3f}\")\n",
    "print(f\"For n={n_values[-1]}: Mean error = {consistency_results.iloc[-1]['mean_error']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Uniform Maximum Estimator (Biased but Consistent)\n",
    "\n",
    "Using the function from the appendix to demonstrate consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_max(x):\n",
    "    return np.max(x)\n",
    "\n",
    "def generate_uniform(n):\n",
    "    return rng.uniform(0, 3.0, n)  # theta = 3\n",
    "\n",
    "# Test uniform max consistency\n",
    "uniform_results = check_consistency(\n",
    "    uniform_max, 3.0, generate_uniform, n_values, R=2000, epsilon=0.1\n",
    ")\n",
    "\n",
    "print(\"Uniform maximum consistency results:\")\n",
    "print(uniform_results.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot uniform max convergence\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Convergence rate\n",
    "axes[0].plot(uniform_results['n'], uniform_results['convergence_rate'], 'g-', linewidth=3, marker='s')\n",
    "axes[0].axhline(0.95, color='red', linestyle='--', alpha=0.7, label='95% target')\n",
    "axes[0].set_xlabel('Sample Size')\n",
    "axes[0].set_ylabel('Convergence Rate')\n",
    "axes[0].set_title('Uniform Max Consistency\\nP(|max Xᵢ - θ| ≤ 0.1)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Mean error vs sample size\n",
    "axes[1].plot(uniform_results['n'], uniform_results['mean_error'], 'r-', linewidth=3, marker='s')\n",
    "axes[1].set_xlabel('Sample Size')\n",
    "axes[1].set_ylabel('Mean Absolute Error')\n",
    "axes[1].set_title('Uniform Max Error Decreases with Sample Size')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/uniform_max_consistency.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"For n={n_values[-1]}: Convergence rate = {uniform_results.iloc[-1]['convergence_rate']:.3f}\")\n",
    "print(f\"For n={n_values[-1]}: Mean error = {uniform_results.iloc[-1]['mean_error']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inconsistent Estimator: First Observation\n",
    "\n",
    "Demonstrate that using only the first observation is inconsistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_observation(x):\n",
    "    return x[0]  # Always use first observation\n",
    "\n",
    "# Test first observation \"estimator\"\n",
    "inconsistent_results = check_consistency(\n",
    "    first_observation, 5.0, generate_normal, n_values, R=2000, epsilon=0.1\n",
    ")\n",
    "\n",
    "print(\"First observation 'estimator' results:\")\n",
    "print(inconsistent_results.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison of consistent vs inconsistent\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Convergence rate comparison\n",
    "axes[0].plot(consistency_results['n'], consistency_results['convergence_rate'], 'b-', linewidth=3, marker='o', label='Sample Mean')\n",
    "axes[0].plot(uniform_results['n'], uniform_results['convergence_rate'], 'g-', linewidth=3, marker='s', label='Uniform Max')\n",
    "axes[0].plot(inconsistent_results['n'], inconsistent_results['convergence_rate'], 'r-', linewidth=3, marker='^', label='First Obs')\n",
    "axes[0].axhline(0.95, color='black', linestyle='--', alpha=0.7, label='95% target')\n",
    "axes[0].set_xlabel('Sample Size')\n",
    "axes[0].set_ylabel('Convergence Rate')\n",
    "axes[0].set_title('Consistent vs Inconsistent Estimators')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Mean error comparison\n",
    "axes[1].plot(consistency_results['n'], consistency_results['mean_error'], 'b-', linewidth=3, marker='o', label='Sample Mean')\n",
    "axes[1].plot(uniform_results['n'], uniform_results['mean_error'], 'g-', linewidth=3, marker='s', label='Uniform Max')\n",
    "axes[1].plot(inconsistent_results['n'], inconsistent_results['mean_error'], 'r-', linewidth=3, marker='^', label='First Obs')\n",
    "axes[1].set_xlabel('Sample Size')\n",
    "axes[1].set_ylabel('Mean Absolute Error')\n",
    "axes[1].set_title('Error Behavior Comparison')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/consistency_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Consistency comparison complete\")\n",
    "print(f\"Sample mean (n={n_values[-1]}): Convergence rate = {consistency_results.iloc[-1]['convergence_rate']:.3f}\")\n",
    "print(f\"Uniform max (n={n_values[-1]}): Convergence rate = {uniform_results.iloc[-1]['convergence_rate']:.3f}\")\n",
    "print(f\"First obs (n={n_values[-1]}): Convergence rate = {inconsistent_results.iloc[-1]['convergence_rate']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MLE and MoM Consistency (Lesson 2 Connection)\n",
    "\n",
    "Show that estimators from Lesson 2 are consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bernoulli parameter estimation (both MLE and MoM)\n",
    "def bernoulli_mle(x):\n",
    "    return np.mean(x)  # MLE and MoM coincide for Bernoulli\n",
    "\n",
    "def generate_bernoulli(n):\n",
    "    return rng.binomial(1, 0.7, n)  # p = 0.7\n",
    "\n",
    "# Test Bernoulli estimator consistency\n",
    "bernoulli_results = check_consistency(\n",
    "    bernoulli_mle, 0.7, generate_bernoulli, n_values, R=2000, epsilon=0.05\n",
    ")\n",
    "\n",
    "print(\"Bernoulli estimator consistency results:\")\n",
    "print(bernoulli_results.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Bernoulli estimator convergence\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(bernoulli_results['n'], bernoulli_results['convergence_rate'], 'purple', linewidth=3, marker='D')\n",
    "plt.axhline(0.95, color='red', linestyle='--', alpha=0.7, label='95% target')\n",
    "plt.xlabel('Sample Size')\n",
    "plt.ylabel('Convergence Rate')\n",
    "plt.title('Bernoulli MLE/MoM Consistency\\nP(|p̂ - p| ≤ 0.05)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/bernoulli_consistency.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Bernoulli estimator (n={n_values[-1]}): Convergence rate = {bernoulli_results.iloc[-1]['convergence_rate']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary and Key Takeaways\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. Sample means are consistent by the Law of Large Numbers (Lesson 1)\n",
    "2. Biased estimators (like uniform max) can still be consistent\n",
    "3. Inconsistent estimators fail because variance doesn't vanish\n",
    "4. MLE and MoM estimators from Lesson 2 are typically consistent\n",
    "\n",
    "Key insights:\n",
    "- Consistency requires vanishing variance, not just correct centering\n",
    "- The LLN (Lesson 1) guarantees consistency of sample means\n",
    "- Lesson 2 estimators inherit consistency from the LLN\n",
    "- Visual inspection of convergence rates helps understand consistency"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "applied-stats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
