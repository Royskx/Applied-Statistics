% !TeX program = xelatex
\documentclass{beamer}

% Theme
\usetheme{Madrid}
\usecolortheme{default}
\setbeamertemplate{navigation symbols}{}
% Allow frame titles to wrap with proper spacing and positioning
\setbeamerfont{frametitle}{size=\large}
\setbeamertemplate{frametitle}{%
  \nointerlineskip
  \begin{beamercolorbox}[wd=\paperwidth,sep=0.3cm,left]{frametitle}%
    \usebeamerfont{frametitle}\insertframetitle\par
  \end{beamercolorbox}%
}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath, amssymb, bm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{arydshln}
\usepackage{hyperref}
\usepackage{centernot}
\usepackage{mathtools}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,calc}

% Ensure figures auto-fit within slides by default (consistent with Lesson 1)
\setkeys{Gin}{width=\linewidth,height=0.62\textheight,keepaspectratio}

% Sanitize PDF bookmarks: replace math/special macros with plain-text fallbacks (consistent with Lesson 1)
\pdfstringdefDisableCommands{%%
  \def\textemdash{-}%%
  \def\P{P}%%
  \def\E{E}%%
  \def\Var{Var}%%
  \def\ell{ell}%%
  \def\mathbb#1{#1}%%
  \def\mathcal#1{#1}%%
  \def\bm#1{#1}%%
}

% Notation and helpers (aligned with Lessons 00/01)
\newcommand{\R}{\mathbb{R}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\1}{\mathbf{1}}
\newcommand{\indep}{\perp\!\!\!\perp}
\newcommand{\toP}{\xrightarrow{\,\mathsf{P}\,}}
\newcommand{\toas}{\xrightarrow{\,\mathsf{a.s.}\,}}
\newcommand{\tod}{\xrightarrow{\,\mathcal{D}\,}}

% Helper for robust top-line commands (see lessons/safe-latex-edits.md)
\newcommand{\robustcmd}[1]{\csname #1\endcsname}
\providecommand{\textbf}[1]{\robustcmd{textbf}{#1}}

% Section TOC at start of each section
\AtBeginSection[]{
  \begin{frame}{Outline}
    \robustcmd{tableofcontents}[currentsection,hideothersubsections]
  \end{frame}
}

% Title
\robustcmd{title}{Lesson 2 \textemdash{} Statistical Learning: Parameter Estimation}
\robustcmd{subtitle}{MLE, Method of Moments, Fisher Information, Uncertainty}
\robustcmd{author}{Applied Statistics Course}
\robustcmd{date}{\today}

\begin{document}

\begin{frame}
  \robustcmd{titlepage}
\end{frame}

\begin{frame}{Learning Objectives}
  \begin{itemize}
    \setlength{\itemsep}{0.4em}
    \item Derive estimators using \textbf{maximum likelihood} and \textbf{method of moments}.
    \item Compute \textbf{standard errors} via Fisher information and the \textbf{delta method}.
    \item Assess estimators: \textbf{bias}, \textbf{variance}, \textbf{MSE}; use \textbf{likelihood profiles} and \textbf{bootstrap}.
    \item Implement simulation studies to compare procedures.
  \end{itemize}
  {\footnotesize Recall Lesson 1: random variables, laws (PMF/PDF/CDF), LLN/CLT, and notation (\(\P,\E,\Var\)).}
\end{frame}

% ============================================================================
% Section 1: Maximum Likelihood Estimation - Likelihood Concepts
% ============================================================================
\section{Maximum Likelihood Estimation (MLE)}
\input{sections/01-mle-likelihood}

% ============================================================================
% Section 2: Maximum Likelihood Estimation - Examples and Properties
% ============================================================================
\input{sections/02-mle-properties}

% ============================================================================
% Section 3: Method of Moments
% ============================================================================
\section{Method of Moments}
\input{sections/03-method-of-moments}

% \section{Fisher Information and SE}

% \begin{frame}{Fisher Information and Standard Errors}
%   \begin{block}{Definitions}
%     Fisher information: \(I(\theta) = \E\big[-\tfrac{\partial^2}{\partial\theta^2} \ell(\theta)\big]\). SE: \(\operatorname{SE}(\hat\theta) \approx \sqrt{I(\hat\theta)^{-1}/n}.\)
%   \end{block}
%   \begin{block}{Delta method}
%     \small If \(\sqrt{n}(\hat\theta-\theta)\tod\mathcal N(0,\Sigma)\), then \(\sqrt{n}(g(\hat\theta)-g(\theta))\tod\mathcal N(0,\nabla g\,\Sigma\,\nabla g^\top).\)
%   \end{block}
%   \begin{exampleblock}{}
%     \begin{columns}[T]
%       \column{0.48\linewidth}
%       {\centering\includegraphics{figures/fig_bern_loglik.png}\\[-1em]
%       \scriptsize Likelihood.}
%       \column{0.48\linewidth}
%       {\centering\includegraphics{figures/fig_delta_log_var.png}\\[-1em]
%       \scriptsize Delta method.}
%     \end{columns}
%   \end{exampleblock}
% \end{frame}

% \begin{frame}{Score equations and Newton--Raphson}
%   \begin{block}{Score}
%     The score is \(U(\theta)=\partial \ell(\theta)/\partial\theta\). MLEs solve the \textbf{score equations} \(U(\hat\theta)=0\).
%   \end{block}
%   \begin{exampleblock}{Newton--Raphson (scalar)}
%     Initialize \(\theta^{(0)}\). Iterate \(\theta^{(t+1)} = \theta^{(t)} - \dfrac{U(\theta^{(t)})}{U'(\theta^{(t)})}\). In multiple dimensions, replace derivatives with gradient and Hessian.
%   \end{exampleblock}
%   \begin{block}{Asymptotic normality (proof sketch)}
%     Taylor expand the score around \(\theta_0\): \(0=U(\hat\theta)\approx U(\theta_0)+U'(\theta_0)(\hat\theta-\theta_0)\). Since \(U(\theta_0)=\mathcal{O}_p(\sqrt n)\) and \(-n^{-1}U'(\theta_0)\to I(\theta_0)\), it follows that \(\sqrt n (\hat\theta-\theta_0)\tod \mathcal N(0, I^{-1})\).
%   \end{block}
% \end{frame}

% \begin{frame}{Exponential families}
%   \begin{block}{Form}
%     \(f(x\mid\eta)=h(x)\exp\{\eta^\top T(x)-A(\eta)\}\), with natural parameter \(\eta\), sufficient statistic \(T\), log-partition \(A\).
%   \end{block}
%   \begin{itemize}
%     \item \(\ell(\eta)=\sum_i \eta^\top T(x_i) - nA(\eta)+\text{const}\), concave in \(\eta\). MLE solves \(\nabla A(\hat\eta)=\bar T\).
%     \item Fisher information: \(I(\eta)=n\,\nabla^2 A(\eta)=n\,\Var_\eta[T(X)]\).
%   \end{itemize}
%   \begin{exampleblock}{Examples}
%     Bernoulli, Poisson, Normal with known variance (for the mean), Gamma with fixed shape, etc.
%   \end{exampleblock}
% \end{frame}

% \begin{frame}{Cram\'er--Rao lower bound (CRLB)}
%   \begin{block}{Bound}
%     For any unbiased estimator \(\tilde\theta\) of scalar \(\theta\): \(\Var(\tilde\theta) \ge \dfrac{1}{n\,I(\theta)}\).
%   \end{block}
%   \begin{itemize}
%     \item Equality holds for \textbf{efficient} estimators; asymptotically, MLE attains the bound under regularity.
%     \item Multivariate version: \(\operatorname{Cov}(\tilde\theta) \succeq (n I(\theta))^{-1}\).
%   \end{itemize}
%   \begin{exampleblock}{Implication}
%     The information \(I(\theta)\) sets a fundamental precision limit; designs that increase \(I\) (e.g., larger sample sizes, informative data) reduce variance.
%   \end{exampleblock}
% \end{frame}

% \section{Model Assessment}

% \begin{frame}{Assessing Estimators and Models}
%   \begin{itemize}
%     \item \textbf{Bias--Variance--MSE}: MSE = Bias$^2$ + Variance.
%     \item \textbf{Likelihood profiles}: visualize curvature near \(\hat\theta\) to gauge uncertainty.
%     \item \textbf{Parametric bootstrap}: resample from fitted model; approximate SE and CIs.
%     \item Regularization and model selection are covered later in the course.
%   \end{itemize}
%   \begin{block}{Parametric bootstrap (algorithm)}
%     \small Fit \(\hat\theta\). For \(b=1,\dots,B\): simulate \(x^{(b)}\sim f(\cdot\mid\hat\theta)\); compute \(\hat\theta^{(b)}\). Use the empirical sd of \(\{\hat\theta^{(b)}\}\) as \(\widehat{\operatorname{SE}}\); percentiles for CIs.
%   \end{block}
% \end{frame}

% \section{Worked Examples}

% \begin{frame}{Example: Exponential(\(\lambda\))}
%   \begin{itemize}
%     \item MLE: \(\hat\lambda=1/\bar x\); asymptotic SE: \(\operatorname{SE}(\hat\lambda)\approx \hat\lambda/\sqrt{n}.\)
%     \item Compare with MoM: solve \(\E[X]=1/\lambda\Rightarrow \tilde\lambda=1/\bar x\) (same here).
%   \end{itemize}
%   \begin{block}{Derivation}
%     \small\raggedright \(\ell(\lambda)=n\log\lambda-\lambda\sum x_i\Rightarrow \partial\ell/\partial\lambda=\tfrac{n}{\lambda}-\sum x_i=0 \Rightarrow \hat\lambda=1/\bar x.\) Info: \(I(\lambda)=n/\lambda^2\).
%   \end{block}
% \end{frame}

% \begin{frame}{Example: Normal(\(\mu,\sigma^2\))}
%   Joint MLEs: \(\hat\mu=\bar x\), \(\widehat{\sigma^2}=\tfrac{1}{n}\sum (x_i-\bar x)^2\) (biased). Unbiased variance uses \(\tfrac{1}{n-1}\).
%   \medskip
%   \begin{itemize}
%     \item Asymptotic var: \(\Var(\hat\mu)\approx \sigma^2/n\); \(\Var(\widehat{\sigma^2})\) from information or delta method.
%   \end{itemize}
%   \begin{block}{Derivation sketch}
%     \small\raggedright \(\ell(\mu,\sigma^2)=-\tfrac{n}{2}\log(2\pi\sigma^2)-\tfrac{1}{2\sigma^2}\sum (x_i-\mu)^2\). Partial derivatives yield the stated MLEs.
%   \end{block}
% \end{frame}

% \section{Exercises and Practical}

% \begin{frame}{Short Exercises}
%   \small
%   \begin{enumerate}
%     \item Binomial(\(n,p\)), known \(n\): show MLE \(\hat p=\bar x/n\) equals MoM.
%     \item Uniform(0,\(\theta\)): derive MLE and discuss the bias of the maximum statistic.
%     \item Delta method: if \(\hat\theta\sim \mathcal N(\theta,\sigma^2/n)\), derive \(\Var(\log \hat\theta)\) to first order.
%   \end{enumerate}
%   \begin{alertblock}{Hint for 2}
%     \small Order statistic \(X_{(n)}=\max X_i\) has CDF \((x/\theta)^n\) on \([0,\theta]\); compute \(\E[X_{(n)}]\) to analyze bias.
%   \end{alertblock}
% \end{frame}

% \begin{frame}{Practical Preview}
%   We'll simulate to compare MLE and MoM and use bootstrap for SEs.
%   \begin{itemize}
%     \item Keep code deterministic (set a RNG seed).
%     \item Use \texttt{numpy}, \texttt{scipy}, and \texttt{pandas} as needed. See course style guide.
%   \end{itemize}
%   \begin{exampleblock}{Figure generation}
%     \small\raggedright Likelihood profile and delta-method visuals are generated by \texttt{figures/make\_figures.py} (Python).
%   \end{exampleblock}
% \end{frame}

% \begin{frame}{Summary}
%   \begin{itemize}
%     \item MLE and MoM provide complementary routes to estimation.
%     \item Fisher information and the delta method quantify uncertainty.
%     \item Simulation and bootstrap help validate asymptotics.
%   \end{itemize}
% \end{frame}

\end{document}
